{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2: Basic Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "Run the `%load` command, then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load ../setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "This topic (Topic 2) and Topic 3 concern the task of sentiment analysis. You will be using a corpus of **book reviews** within an **Amazon review corpus**.\n",
    "\n",
    "You be exploring various techniques that can be used to classify the sentiment of Amazon book reviews as either positive or negative. \n",
    "\n",
    "You will be developing your own **Word List** classifiers and then comparing them to the **NLTK Na√Øve Bayes** classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing sets\n",
    "During the next two lab sessions you will be training and testing various document classifiers. It is essential that the data used in the testing phase is not used during the training phase, since this can lead to overestimating performance. \n",
    "\n",
    "We now introduce the `split_data` function (defined in the cell below) which can be used to get separate **training** and **testing** sets.\n",
    "\n",
    "### Exercise\n",
    "Look through the code in the following cell, reading the comments and making sure that you understand each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample # have a look at https://docs.python.org/3/library/random.html to see what random.sample does\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "\n",
    " \n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the \n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = list(data) # data is a generator, so this puts all the generated items in a list\n",
    " \n",
    "    n = len(data)  #Found out number of samples present\n",
    "    train_indices = sample(range(n), int(n * ratio))          #Randomly select training indices\n",
    "    test_indices = list(set(range(n)) - set(train_indices))   #Randomly select testing indices\n",
    " \n",
    "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
    "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
    " \n",
    "    return (train, test)                       #Return split data\n",
    " \n",
    "#Create an Amazon corpus reader pointing at only book reviews\n",
    "book_reader = AmazonReviewCorpusReader().category(\"dvd\")\n",
    "\n",
    "#The following two lines use the documents function on the Amazon corpus reader. \n",
    "#This returns a generator over reviews in the corpus. \n",
    "#Each review is an instance of a Python class called AmazonReview. \n",
    "#An AmazonReview object contains all the data about a review.\n",
    "pos_train, pos_test = split_data(book_reader.positive().documents())\n",
    "neg_train, neg_test = split_data(book_reader.negative().documents())\n",
    "\n",
    "#You can also combine the training data\n",
    "train = pos_train + neg_train\n",
    "test = pos_test + neg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Make a copy of the cell above and position it below this cell. Then adapt this code so that it splits the book review corpus in various ways (i.e. different test/training ratios), and by measuring the size of the resulting splits, check that the size of both splits match the specified ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/split_checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating word lists\n",
    "The next section will explain how to use a sentiment classifier that bases its decisions on word lists. The classifier requires a list of words indicating positive sentiment, and a second list of words indicating negative sentiment. Given positive and negative word lists, a document's overall sentiment is determined based on counts of occurrences of words that occur in the two lists. In this section we are concerned with the creation of the word lists. We will be considering both hand-crafted lists and automatically generated lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Create a reasonably long hand-crafted list of words that you think indicate positive sentiment.\n",
    "- Create a reasonably long hand-crafted list of words that indicate negative sentiment.\n",
    "\n",
    "Use the following cells to store these lists in the variables `my_positive_word_list` and `my_negative_word_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_positive_word_list = [\"good\",\"great\",\"lovely\"] # put your own list here\n",
    "my_negative_word_list = [\"bad\", \"terrible\", \"awful\"] # put your own list here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you should try to derive word lists from the data. One way to do this, is to use the most frequent words in positive reviews as your positive list, and the most frequent words in negative reviews as your negative list. This can be done with the [NLTK <code style=\"background-color: #F5F5F5;\">FreqDist</code>](http://www.nltk.org/api/nltk.html#module-nltk.probability) object. \n",
    "\n",
    "### Exercise\n",
    "Make sure that you understand the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from functools import reduce # see https://docs.python.org/3/library/functools.html\n",
    "\n",
    "#Helper function. Given a list of reviews, return a list of all the words in those reviews\n",
    "#To understand this look at the description of functools.reduce in https://docs.python.org/3/library/functools.html\n",
    "def get_all_words(amazon_reviews):\n",
    "    return reduce(lambda words,review: words + review.words(), amazon_reviews, [])\n",
    "\n",
    "#A frequency distribution over all words in positive book reviews\n",
    "pos_freqdist = FreqDist(get_all_words(pos_train))\n",
    "neg_freqdist = FreqDist(get_all_words(neg_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank code cell below write code that uses the frequency lists, `pos_freqdist` and `neg_freqdist`, created in the above cell and `my_positive_word_list` and `my_negative_word_list` that you manually created earlier to determine whether or not the review data conforms to your expectations. In particular, whether:\n",
    "- the words you expected to indicate positive sentiment actually occur more frequently in positive reviews than negative reviews\n",
    "- the words you expected to indicate negative sentiment actually occur more frequently in negative reviews than positive reviews.\n",
    "\n",
    "Display your findings in a table using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a partial solution\n",
    "#%load solutions/check_expectations_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/check_expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The cell below is a copy of the cell that appeared just before the last exercise.\n",
    "\n",
    "Add two functions to this code.\n",
    "\n",
    "- `most_frequent_words` - this function should take two arguments: a frequency distribution and a natural number, k. It should return the top k most frequent  words in the frequency distribution. You can use the `most_common` method for this, but you will need to aware of what this method returns.\n",
    "- `words_above_threshold` - this function also takes two arguments: a frequency distribution and a natural number, k. It should return all of the words that have a frequency greater than k.\n",
    "\n",
    "Remove punctuation and stopwords from consideration. You can re-use code from near the end of Topic 1 notebook.\n",
    "Using the training data, create two sets of positive and negative word lists using these functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from functools import reduce # see https://docs.python.org/3/library/functools.html\n",
    "\n",
    "#Helper function. Given a list of reviews, return a list of all the words in those reviews\n",
    "#To understand this look at the description of functools.reduce in https://docs.python.org/3/library/functools.html\n",
    "def get_all_words(amazon_reviews):\n",
    "    return reduce(lambda words,review: words + review.words(), amazon_reviews, [])\n",
    "\n",
    "#A frequency distribution over all words in positive book reviews\n",
    "pos_freqdist = FreqDist(get_all_words(pos_train))\n",
    "neg_freqdist = FreqDist(get_all_words(neg_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "# %load solutions/make_word_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a word list based classifier\n",
    "Now you have a number of word lists for use with a classifier. The following code can be used as the basis for creating a word list based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI): \n",
    "\n",
    "    def __init__(self, pos, neg): \n",
    "        self._pos = pos \n",
    "        self._neg = neg \n",
    "\n",
    "    def classify(self, words): \n",
    "        score = 0\n",
    "        \n",
    "        # add code here that assigns an appropriate value to score\n",
    "        \n",
    "        return \"N\" if score < 0 else \"P\" \n",
    "\n",
    "    def batch_classify(self, docs): \n",
    "        return [self.classify(doc.words() if hasattr(doc, 'words') else doc) for doc in docs] \n",
    "\n",
    "    def labels(self): \n",
    "        return (\"P\", \"N\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(top_pos, top_neg)\n",
    "classifier.classify(\"I read the book\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Copy the above code cell and move it to below this one. Then complete the `classify` method in the above code as specified below.\n",
    "- Test your classifier on several very simple hand-crafted examples to verify that you have implemented `classify` correctly.\n",
    "\n",
    "The classifier is initialised with a list of positive words, and a list of negative words. The words of a document are passed to the `classify` method (which is partially completed in the above code fragment). The `classify` method should be defined so that each occurrence of a negative word decrements `score`, and each occurrence of a positive word increments `score`. \n",
    "- For `score` less than 0, an \"`N`\" for negative should be returned.\n",
    "- For `score` greater than 0,  \"`P`\" for positive should returned.\n",
    "- For `score` of 0, the classification decision should be made randomly (see https://docs.python.org/3/library/random.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/simple_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating word list based classifier\n",
    "Below is code that uses an evaluation function in order to determine how well your classifier performs. The function returns the <b>accuracy</b> of a classifier. The accuracy metric is defined as the proportion of documents that were correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.stats import evaluate_wordlist_classifier\n",
    "\n",
    "#Create a new classifier with your words lists\n",
    "book_classifier = SimpleClassifier(top_pos, top_neg)\n",
    "\n",
    "#Evaluate classifier\n",
    "#The function requires three arguments:\n",
    "# 1. Word list based classifer\n",
    "# 2. A list (or generator) of positive AmazonReview objects\n",
    "# 3. A list (or generator) of negative AmazonReview objects\n",
    "score = evaluate_wordlist_classifier(book_classifier, pos_test, neg_test)  \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  \n",
    "\n",
    "Use the blank cell below to perform the following two experiments:\n",
    "- Evaluate the performance of a classifier using hand-crafted lists.\n",
    "- Evaluate the performance of a classifier using lists derived from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/classifier_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na√Øve Bayes classifiers\n",
    "We now turn to a different model of document classification known as Na√Øve Bayes.\n",
    "\n",
    "Before we apply them to Amazon reviews, we need to implement them!\n",
    "\n",
    "We will introduce them through a very simple example dataset involving documents that are either about the weather or football. The classifier will be trained to distinguish these two topics from one another.\n",
    "\n",
    "There are, therefore, two classes `weather` and `football`. The classifier's job is to determine whether a document that it is given is in the class `weather` or in the class `football`.\n",
    "\n",
    "We give the classifier examples of documents in the `weather` class, and examples of documents in the `football` class. For now, to keep things simple, our so-called documents will be very short phrases.\n",
    "\n",
    "Run the following cell to set up some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some test data to get us started. It is deliberately VERY simple. \n",
    "\n",
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "One of the simplying assumptions that is made with Na√Øve Bayes classification is that each document is taken to be a so-called bag of words. What this means is that the word order is ignored, and we are not taking into account the number of times a word appears in a document. Note that there are variants of Na√Øve Bayes where the number of times a words appears in a document is taken into account, but we will not consider that case here.\n",
    "\n",
    "Given the bag-of-words assumption, we will represent each training document as a pair consisting of a dictionary that maps each word that appears in the document to `True`, and a string denoting the class of the document. \n",
    "\n",
    "### Exercise\n",
    "In the cell below, write code that achieves this. Create three dictionaries:\n",
    "- `weather_data_train`: a dictionary containing the data for documents in the class `weather`;\n",
    "- `football_data_train`: a dictionary containing the data for documents in the class `football`;\n",
    "- `train_data`: which is simply `weather_data_train + football_data_train`\n",
    "\n",
    "Hint: this can be done with nested list comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/format_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a Na√Øve Bayes classifier works\n",
    "\n",
    "We will now look at how a NB work, using our `weather` versus `football` classification task.\n",
    "\n",
    "In order to classify a document, $d$, we need to determine which of these probabilities is greatest:\n",
    "\n",
    "$$P(\\,\\mbox{weather}\\,|\\,d) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{football}\\,|\\,d)$$\n",
    "\n",
    "$d$ could, for example, be the string \"today is looking cloudy\", which would give us:\n",
    "\n",
    "$$P(\\,\\mbox{weather}\\,|\\,\\mbox{\"today is looking cloudy\"}) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{football}\\,|\\,\\mbox{\"today is looking cloudy\"})$$\n",
    "\n",
    "The idea is that if the term on the left is higher then the document is in category `weather`, and if the term on the right is higher then the document is in category `football`.\n",
    "\n",
    "$P(X|Y)$ means the probability of $X$ given $Y$. So, $P(\\,\\mbox{weather}\\,|\\,d)$ means the probability, given a document $d$, of $d$ being of class `weather`.\n",
    "\n",
    "We are going to use something called Bayes' rule which states that:\n",
    "\n",
    "$$P(X|Y) = \\frac{P(Y|X)\\cdot P(X)}{P(Y)}$$\n",
    "\n",
    "Applying Bayes' rule to our problem leads to the following comparision:\n",
    "\n",
    "$$\\frac{P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,)}{p(d)} \\qquad\\qquad \\mbox{versus} \\qquad\\qquad \\frac{P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)}{p(d)}$$\n",
    "\n",
    "Since both sides are being divided by the same thing, we only need to make the following comparision:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)$$\n",
    "\n",
    "Let's just look at what each of these probabilities mean?\n",
    "\n",
    "1. $P(\\,d\\,|\\,\\mbox{weather}\\,)$: this is the probability of a document in the `weather` category being the document $d$\n",
    "\n",
    "2. $P(\\,d\\,|\\,\\mbox{football}\\,)$: this is the probability of a document in the `football` category being the document $d$\n",
    "\n",
    "3. $P(\\,\\mbox{weather}\\,)$: this is the probability of a randomly selected document being of category `weather`.\n",
    "\n",
    "4. $P(\\,\\mbox{football}\\,)$: this is the probability of a randomly selected document being of category `football`.\n",
    "\n",
    "How are we going to obtain these probabilities? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class priors\n",
    "\n",
    "We have established that we need to know $P(\\,\\mbox{weather}\\,)$ and $P(\\,\\mbox{football}\\,)$. These are called the class priors. Let's see how we can obtain (estimated) values for these probabilities from our training data.\n",
    "\n",
    "The classifier has seen three documents of class `weather` and two documents of class `football`.\n",
    "\n",
    "From this it learns that `weather` documents are slightly more common that `football` documents, and it therefore has a slight bias towards saying a document is a `weather` document.\n",
    "\n",
    "To be more precise, the classifier has learned that:\n",
    "- The probability that a document is in class `weather` is $3/5$.  \n",
    "We say $P(\\mbox{weather})=3/5=0.6$.\n",
    "\n",
    "- The probability that a document is in class `football` is $2/5$.  \n",
    "We say $P(\\mbox{football})=2/5=0.4$.\n",
    "\n",
    "In general, if the training data contained $n_1$ documents of class `weather` and $n_2$ documents of class `football`, then \n",
    "\n",
    "$$P(\\mbox{weather})=\\frac{n_1}{n_1+n_2} \\qquad \\mbox{and} \\qquad P(\\mbox{football})=\\frac{n_2}{n_1+n_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In blank the cell below, implement a function `class_priors(training_data)` that takes a dictionary of training data  and returns a dictionary that maps the name of each class to the class prior for that class.\n",
    "\n",
    "Once you have done this, test it out on the training data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/class_priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probabilities\n",
    "We now turn to the problem of how to calculate (estimates of) the probabilities such as $P(\\,d\\,|\\,\\mbox{weather}\\,)$ and $P(\\,d\\,|\\,\\mbox{football}\\,)$, for some document $d$. The problem we have is that $d$ is a document, potentially a long document, that we won't have seen in the training data.\n",
    "\n",
    "To address this, the Na√Øve Bayes model of document classification makes a major simplifying assumption.  In particular, it is assumed that the probabiity that different words occur in a document are independent of one another.\n",
    "\n",
    "For example, if $d=\\mbox{\"today is looking cloudy\"}$ then this assumption tells us that:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,\\mbox{\"today is looking cloudy\"}\\,|\\,\\mbox{weather}\\,) &=& P(\\{\\mbox{\"today\"},\\mbox{\"is\"},\\mbox{\"looking\"},\\mbox{\"cloudy\"}\\}\\,|\\,\\mbox{weather}\\,)\\\\\n",
    "&=& P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For the general case, with class $c$ and document $d=\\{w_1,\\ldots,w_n\\}$, we have:\n",
    "\\begin{eqnarray*}\n",
    "P(\\,d\\,|\\,c\\,) &=& P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,)\\\\\n",
    "&=& \\prod_{i=1}^n P(\\,w_i\\,|\\,c\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The point is that it is plausible that given a reasonable amount of training data, we can make reasonable estimates of the probabilities $P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)$, and  $P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)$. \n",
    "\n",
    "We not look at how that is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating conditional probabilities\n",
    "So we have established that we need estimates of probabilities such as: $P(\\,\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)$, and $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)$.\n",
    "\n",
    "How can these probabilities be estimated from the training data?\n",
    "\n",
    "Look at the training data we set up above. There are 3 documents of class `weather` and within these documents there are a total of 11 tokens (8 different types).\n",
    "\n",
    "From the data we can estimate the following probabilities:\n",
    "- the probability of seeing \"today\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"today\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"it\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"it\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"is\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"raining\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"raining\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"looking\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"cloudy\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"nice\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"nice\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$; and\n",
    "- the probability of seeing \"weather\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"weather\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$\n",
    "- the probability of seeing any other word in a `weather` document is 0.\n",
    "Notice that all of these conditional probabilities sum to $1$.\n",
    "\n",
    "We can do the same thing for the `football` documents:\n",
    "- the probability of seeing \"city\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"city\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"looking\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"good\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"good\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"advantage\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"advantage\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"united\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"united\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing any other word in a `football` document is 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "We now look at how to implement the calculation of conditional probabilties.\n",
    "\n",
    "In the empty cell below, define a function `cond_probs(training_data)` that takes training data and returns a dictionary that maps the name of a class, $c$, onto a dictionary that maps each word, $w$ to the conditional probability for that word given that class, i.e. $\\log(P(w|c))$.\n",
    "\n",
    "Hint: the following line will create a dictionary of the required form:  \n",
    "`c_probs = collections.defaultdict(lambda: defaultdict(float))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a paritial solution\n",
    "#%load solutions/cond_probs_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/cond_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the conditional probability of a document\n",
    "\n",
    "We have created implementations of the following functions:\n",
    "- `class_priors(training_data)` that computes estimates of the class priors from training data;\n",
    "- `cond_probs(training_data)` that computes estimates of the conditional probability of a word given a class from training data\n",
    "\n",
    "Let us suppose that we have applied these functions to our training data as follows.\n",
    "\n",
    "```\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "```\n",
    "\n",
    "`c_priors` and `c_probs` define the classifier.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, complete the function `classify(doc,c_priors,c_probs)`. It should return the class that the classifier assigns to the document, `doc`. \n",
    "\n",
    "In the event of a tie, the function should randomly chose one of the classes (see `random.choice`). \n",
    "\n",
    "- Write your function in a way that allows for the possibilty of any number of classes.\n",
    "- Assume that the document, `doc`, is represented as a dictionary that maps words (in the document) to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(doc,priors,c_probs):\n",
    "\n",
    "    <put your definition of classify here>\n",
    "    \n",
    "\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "sent = \"looking cloudy today\"\n",
    "doc = dict([(word, True) for word in sent.split()])\n",
    "classify(doc,c_priors,c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/my_NB_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A problem\n",
    "There is a problem with the classifier that we have written. \n",
    "\n",
    "### Exercise\n",
    "Run the classifier on several examples to see if you can discover what the problem is. \n",
    "- You might need to run the same document through the classifier several times to see if you get different class for different runs\n",
    " - If you've implemented `classify` correctly then this happens when you have a tie.\n",
    "- Try the sentence \"city looking cloudy today\"\n",
    "\n",
    "What is going wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one smoothing\n",
    "It will often be the case that a word appears in documents of one class, but not in any documents of another class.\n",
    "- We saw that with the word \"city\" in the document \"city looking cloudy today\".\n",
    "- While \"city\" appears in documents of class `football`, it does not appear in any documents of class `weather`. \n",
    "- Thus, the conditional probabiity $P(\\,\\mbox{city}\\,|\\,\\mbox{weather}\\,)$ is equal to zero.\n",
    "- We are **multiplying** probabilities, so the document ends up with a score of zero even though all of the other words in the document suggest that it is of class `weather`.\n",
    "\n",
    "To get around this we need to do something called **smoothing** in order to avoid zero probabilities. \n",
    "\n",
    "In particular, we will implement a version of smoothing called **add-one smoothing**. This involves adding a count of one to all of the known vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known vocabulary\n",
    "The words that appear in documents in the training data are collectively described as the known vocabulary. These are the words that the classifier can learn something about. If the classifier is asked to classify a document that contains any words that are not in the known vocabulary then the classifier will simply ignore them.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, write a function `known_vocabulary(training_data)` that takes some training and returns a set containing all of words that appear in documents in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing add-one smoothing\n",
    "As the name suggests, add-one smoothing involves adding counts.\n",
    "\n",
    "In particular, for each word, $w$, in the known vocabulary and each class, $c$, we add one extra count to our record of how many times $w$ appears in documents of class $c$. We are, in effect, hallucinating counts. The reason for doing this is that it means that we avoid zero probabilties.\n",
    "\n",
    "### Exercise\n",
    "In the blank cell below copy in your code for the `cond_probs` function that you wrote earlier. Then adapt this code so that it implements the add-one smoothing scheme.\n",
    "- You will find it useful to use your `known_vocabulary` function.\n",
    "- If there are $k$ words in your known vocabulary, then you will add $k$ counts for each class. \n",
    "- Therefore, when calculating conditional probabilities, you need to add $k$ to the denominator to account for these extra counts.\n",
    "\n",
    "Test out your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/smoothed_cond_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing class priors\n",
    "It is possible (though perhaps unlikely) that the training data does not contain any data for one class. In order to take care of this, we also smooth the class priors.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, revise the `class_priors` function so that it adds one to the count of each class.\n",
    "\n",
    "Check that the sum of the priors for each of the classes is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/smoothed_class_priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignoring OOV words\n",
    "We now look at how to update the `classify` function that we wrote earlier so that it ignores out of vocabulary words that appear in a document being classified.\n",
    "\n",
    "### Exercise\n",
    "In the blank cell below, copy the `classify` method you wrote earlier and update it so that words not in the known vocabulary are ignored.\n",
    "- You will want to add an additional argument to the `classify` function that is a set containing the known vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/new_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underflow\n",
    "\n",
    "We need to address one final problem concerning the multiplication of probabilities.\n",
    "\n",
    "Recall this equation from earlier:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,d\\,|\\,c\\,) &=& P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,)\\\\\n",
    "&=& \\prod_{i=1}^n P(\\,w_i\\,|\\,c\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This tells us that in order to compute $P(\\,d\\,|\\,c\\,)$ for some document $d$ and class $c$, we must multiply $n$ conditional probabilities, one for each word in the document.\n",
    "\n",
    "While in our toy example, this is not an issue. However, in a more realistic settings, where we had thousands of documents, each of which contained multiple paragraphs, we would find ourselves multiplying large numbers of very small probabilities. This would lead to **underflow**.\n",
    "\n",
    "To avoid ths problem, we will add the log of probabilties. \n",
    "\n",
    "To understand why this is a reasonable thing to do let us recall a comparison from earlier:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)$$\n",
    "\n",
    "Our goal is to determine which of the values (on the left and right) is larger (or determine that they are equal). \n",
    "\n",
    "It should be clear that we will get exactly the same answer to this question by making the following comparsion.\n",
    "\n",
    "$$\\log(P(\\,d\\,|\\,\\mbox{weather}\\,)) + \\log(P(\\,\\mbox{weather}\\,)) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad \\log(P(\\,d\\,|\\,\\mbox{football}\\,)) + \\log(P(\\,\\mbox{football}\\,))$$\n",
    "\n",
    "Thus, rather than calculating conditional probabilities as described above, we will calculate log conditional probabilities like this:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log(P(\\,\\mbox{\"today is looking cloudy\"}\\,|\\,\\mbox{weather}\\,)) &=& \\log(P(\\{\\mbox{\"today\"},\\mbox{\"is\"},\\mbox{\"looking\"},\\mbox{\"cloudy\"}\\}\\,|\\,\\mbox{weather}\\,))\\\\\n",
    "&=& \\log(P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,))\\ +\\\\\n",
    "&&\\log(P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,))\\ +\\\\\n",
    "&&\\log(P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,))\\ + \\\\\n",
    "&&\\log(P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,))\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For the general case, with class $c$ and document $d=\\{w_1,\\ldots,w_n\\}$, we have:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log(P(\\,d\\,|\\,c\\,)) &=& \\log(P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,))\\\\\n",
    "&=& \\sum_{i=1}^n \\log(P(\\,w_i\\,|\\,c\\,))\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank cell below, make a copy of the cell containing the definition of `classify`.\n",
    "\n",
    "Adapt the code so that it adds logs of probabilties rather than multiplies probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/classify_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Na√Øve Bayes classifier on test data\n",
    "We are now ready to run our Na√Øve Bayes classifier on a set of test data. When we do this we want to return the accuracy of the classifier on that data, where accuracy is calculated as follows:\n",
    "\n",
    "$$\\frac{\\mbox{number of test documents that the classifier classifiers correctly}}\n",
    "{\\mbox{total number of test documents}}$$\n",
    "\n",
    "In order to compute this accuracy score, we need to give the classifier **labelled** test data.\n",
    "- This will be in the same format as the training data.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, we set up 5 test documents in the class `weather` and 5 documents in the class `football`.\n",
    "\n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "]\n",
    "\n",
    "weather_data_train = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_train] \n",
    "football_data_train = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_train]\n",
    "train_data = weather_data_train + football_data_train\n",
    "\n",
    "weather_sents_test = [\n",
    "    \"the weather today is nice\",\n",
    "    \"it is raining cats and dogs\",\n",
    "    \"the weather here is wet\",\n",
    "    \"it was hot today\",\n",
    "    \"rain due tomorrow\",\n",
    "]\n",
    "\n",
    "football_sents_test = [\n",
    "    \"what a great goal that was\",\n",
    "    \"poor defending by the city center back\",\n",
    "    \"wow he missed a sitter\",\n",
    "    \"united are a shambles\",\n",
    "    \"shots raining down on the keeper\",\n",
    "]\n",
    "\n",
    "weather_data_test = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_test] \n",
    "football_data_test = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_test]\n",
    "test_data = weather_data_test + football_data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the cell below Implement a `NB_evaluate` function that returns the accuracy of a classifier on a set of labelled test data.\n",
    "`NB_evaluate` should make use of the `classify` function, and take the following arguments:\n",
    "- the test data\n",
    "- the class priors\n",
    "- the conditional probabilities\n",
    "- the known vocabulary (though this is redundant since it could be computed from the conditional probabilities)\n",
    "\n",
    "`NB_evaluate` should return the accuracy of the classifier on the test data.\n",
    "\n",
    "Try out your `NB_evaluate` function on the test data in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/NB_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try out the classifier on different training and test examples.\n",
    "\n",
    "Before running each test try to anticipate what you think the outcome will be.\n",
    "- What happens when none of the words in the test document are in the known vocabulary?\n",
    "- Can you set up data so that there is a tie between the classes, and the classifier randomly chooses a class. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
