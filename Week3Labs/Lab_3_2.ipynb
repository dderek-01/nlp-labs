{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Basic Document Classification (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex NLTK root directory is \\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources\n"
     ]
    }
   ],
   "source": [
    "#necessary library imports and setup introduced previously\n",
    "\n",
    "import sys\n",
    "#sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering') \n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/juliewe/resources')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify.api import ClassifierI\n",
    "\n",
    "from sussex_nltk.corpus_readers import ReutersCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "In labs this week (and next), the focus is on the application of sentiment analysis. You have started using the **Amazon review corpus** and exploring various techniques that can be used to classify the sentiment of Amazon book reviews as either positive or negative. \n",
    "\n",
    "Last time, you developed your own **Word List** classifiers.  This time you will be developing a **Naïve Bayes** classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes classifiers\n",
    "\n",
    "We will introduce Naïve Bayes classifiers through a very simple example dataset involving documents that are either about the weather or football. The classifier will be trained to distinguish these two topics from one another.\n",
    "\n",
    "There are, therefore, two classes `weather` and `football`. The classifier's job is to determine whether a document that it is given is in the class `weather` or in the class `football`.\n",
    "\n",
    "We give the classifier examples of documents in the `weather` class, and examples of documents in the `football` class. For now, to keep things simple, our so-called documents will be very short phrases.\n",
    "\n",
    "Run the following cell to set up some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some test data to get us started. It is deliberately VERY simple. \n",
    "\n",
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "One of the simplying assumptions that is made with Naïve Bayes classification is that each document is taken to be a so-called **bag of words**. What this means is that the word order is ignored. For example, the document phrase \"city looking good\" will generate the same *bag* of words as \"good looking city\" and \"good city looking\"\n",
    "\n",
    "The second assumption that we make is that we use the **multi-variate Bernouilli event model**.   This means that we do not taking into account the number of times a word appears in a document.   Each variable (i.e., word) has two possible values -- `True` if it occurs and `False` if it does not.  Note that there are **multinomial** variants of Naïve Bayes where the number of times a words appears in a document is taken into account, but we will not consider that case here.\n",
    "\n",
    "Given these assumptions, we will represent each training document as a pair consisting of a dictionary that maps each word that appears in the document to `True`, and a string denoting the class of the document. \n",
    "\n",
    "### Exercise 1.1\n",
    "In the cell below, write code that achieves this. Create three lists:\n",
    "- `weather_data_train`: a dictionary containing the data for documents in the class `weather`;\n",
    "- `football_data_train`: a dictionary containing the data for documents in the class `football`;\n",
    "- `train_data`: which is simply `weather_data_train + football_data_train`\n",
    "\n",
    "For example, `weather_data_train` should look like:\n",
    "\n",
    "<img src='files/traindataex.png'>\n",
    "\n",
    "Hint: this can be done with nested list/dictionary comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'today': True, 'it': True, 'is': True, 'raining': True}, 'weather'), ({'looking': True, 'cloudy': True, 'today': True}, 'weather'), ({'it': True, 'is': True, 'nice': True, 'weather': True}, 'weather')]\n"
     ]
    }
   ],
   "source": [
    "def wordDict(words):\n",
    "    return {word: True for word in words}\n",
    "\n",
    "def getDict(train, name):\n",
    "    return [(wordDict(sentence.split()), name) for sentence in train]\n",
    "\n",
    "weather_data_train = getDict(weather_sents_train, 'weather')\n",
    "football_data_train = getDict(weather_sents_train, 'football')\n",
    "train_data = weather_data_train + football_data_train\n",
    "print(weather_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a Naïve Bayes classifier works\n",
    "\n",
    "We will now look at how a NB work, using our `weather` versus `football` classification task.\n",
    "\n",
    "In order to classify a document, $d$, we need to determine which of these probabilities is greatest:\n",
    "\n",
    "$$P(\\,\\mbox{weather}\\,|\\,d) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{football}\\,|\\,d)$$\n",
    "\n",
    "$d$ could, for example, be the string \"today is looking cloudy\", which would give us:\n",
    "\n",
    "$$P(\\,\\mbox{weather}\\,|\\,\\mbox{\"today is looking cloudy\"}) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{football}\\,|\\,\\mbox{\"today is looking cloudy\"})$$\n",
    "\n",
    "The idea is that if the term on the left is higher then the document is in category `weather`, and if the term on the right is higher then the document is in category `football`.\n",
    "\n",
    "$P(X|Y)$ means the probability of $X$ given $Y$. So, $P(\\,\\mbox{weather}\\,|\\,d)$ means the probability, given a document $d$, of $d$ being of class `weather`.\n",
    "\n",
    "We are going to use something called Bayes' rule which states that:\n",
    "\n",
    "$$P(X|Y) = \\frac{P(Y|X)\\cdot P(X)}{P(Y)}$$\n",
    "\n",
    "Applying Bayes' rule to our problem leads to the following comparision:\n",
    "\n",
    "$$\\frac{P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,)}{p(d)} \\qquad\\qquad \\mbox{versus} \\qquad\\qquad \\frac{P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)}{p(d)}$$\n",
    "\n",
    "Since both sides are being divided by the same thing, we only need to make the following comparision:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)$$\n",
    "\n",
    "Let's just look at what each of these probabilities mean?\n",
    "\n",
    "1. $P(\\,d\\,|\\,\\mbox{weather}\\,)$: this is the probability of a document in the `weather` category being the document $d$\n",
    "\n",
    "2. $P(\\,d\\,|\\,\\mbox{football}\\,)$: this is the probability of a document in the `football` category being the document $d$\n",
    "\n",
    "3. $P(\\,\\mbox{weather}\\,)$: this is the probability of a randomly selected document being of category `weather`.\n",
    "\n",
    "4. $P(\\,\\mbox{football}\\,)$: this is the probability of a randomly selected document being of category `football`.\n",
    "\n",
    "How are we going to obtain these probabilities? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class priors\n",
    "\n",
    "We have established that we need to know $P(\\,\\mbox{weather}\\,)$ and $P(\\,\\mbox{football}\\,)$. These are called the class priors. Let's see how we can obtain (estimated) values for these probabilities from our training data.\n",
    "\n",
    "The classifier has seen three documents of class `weather` and two documents of class `football`.\n",
    "\n",
    "From this it learns that `weather` documents are slightly more common that `football` documents, and it therefore has a slight bias towards saying a document is a `weather` document.\n",
    "\n",
    "To be more precise, the classifier has learned that:\n",
    "- The probability that a document is in class `weather` is $3/5$.  \n",
    "We say $P(\\mbox{weather})=3/5=0.6$.\n",
    "\n",
    "- The probability that a document is in class `football` is $2/5$.  \n",
    "We say $P(\\mbox{football})=2/5=0.4$.\n",
    "\n",
    "In general, if the training data contained $n_1$ documents of class `weather` and $n_2$ documents of class `football`, then \n",
    "\n",
    "$$P(\\mbox{weather})=\\frac{n_1}{n_1+n_2} \\qquad \\mbox{and} \\qquad P(\\mbox{football})=\\frac{n_2}{n_1+n_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "In blank the cell below, implement a function `class_priors(training_data)` that takes a dictionary of training data  and returns a dictionary that maps the name of each class to the class prior for that class.\n",
    "\n",
    "Once you have done this, test it out on the training data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probabilities\n",
    "We now turn to the problem of how to calculate (estimates of) the probabilities such as $P(\\,d\\,|\\,\\mbox{weather}\\,)$ and $P(\\,d\\,|\\,\\mbox{football}\\,)$, for some document $d$. The problem we have is that $d$ is a document, potentially a long document, that we won't have seen in the training data.\n",
    "\n",
    "To address this, the Naïve Bayes model of document classification makes a major simplifying assumption.  In particular, it is assumed that the probabiity that different words occur in a document are independent of one another.\n",
    "\n",
    "For example, if $d=\\mbox{\"today is looking cloudy\"}$ then this assumption tells us that:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,\\mbox{\"today is looking cloudy\"}\\,|\\,\\mbox{weather}\\,) &=& P(\\{\\mbox{\"today\"},\\mbox{\"is\"},\\mbox{\"looking\"},\\mbox{\"cloudy\"}\\}\\,|\\,\\mbox{weather}\\,)\\\\\n",
    "&=& P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For the general case, with class $c$ and document $d=\\{w_1,\\ldots,w_n\\}$, we have:\n",
    "\\begin{eqnarray*}\n",
    "P(\\,d\\,|\\,c\\,) &=& P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,)\\\\\n",
    "&=& \\prod_{i=1}^n P(\\,w_i\\,|\\,c\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The point is that it is plausible that given a reasonable amount of training data, we can make reasonable estimates of the probabilities $P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)$, and  $P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)$. \n",
    "\n",
    "We now look at how that is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating conditional probabilities\n",
    "So we have established that we need estimates of probabilities such as: $P(\\,\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)$, and $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)$.\n",
    "\n",
    "How can these probabilities be estimated from the training data?\n",
    "\n",
    "Look at the training data we set up above. There are 3 documents of class `weather` and within these documents there are a total of 11 tokens (8 different types).\n",
    "\n",
    "From the data we can estimate the following probabilities:\n",
    "- the probability of seeing \"today\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"today\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"it\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"it\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"is\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"raining\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"raining\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"looking\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"cloudy\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"nice\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"nice\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$; and\n",
    "- the probability of seeing \"weather\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"weather\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$\n",
    "- the probability of seeing any other word in a `weather` document is 0.\n",
    "Notice that all of these conditional probabilities sum to $1$.\n",
    "\n",
    "We can do the same thing for the `football` documents:\n",
    "- the probability of seeing \"city\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"city\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"looking\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"good\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"good\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"advantage\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"advantage\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"united\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"united\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing any other word in a `football` document is 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "We now look at how to implement the calculation of conditional probabilties.\n",
    "\n",
    "In the empty cell below, define a function `cond_probs(training_data)` that takes training data and returns a dictionary that maps the name of a class, $c$, onto a dictionary that maps each word, $w$ to the conditional probability for that word given that class, i.e. $P(w|c)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the conditional probability of a document\n",
    "\n",
    "We have created implementations of the following functions:\n",
    "- `class_priors(training_data)` that computes estimates of the class priors from training data;\n",
    "- `cond_probs(training_data)` that computes estimates of the conditional probability of a word given a class from training data\n",
    "\n",
    "Let us suppose that we have applied these functions to our training data as follows.\n",
    "\n",
    "```\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "```\n",
    "\n",
    "`c_priors` and `c_probs` define the classifier.\n",
    "\n",
    "### Exercise 1.4\n",
    "In the cell below, complete the function `classify(doc,c_priors,c_probs)`. It should return the class that the classifier assigns to the document, `doc`. \n",
    "\n",
    "In the event of a tie, the function should randomly chose one of the classes (see `random.choice`). \n",
    "\n",
    "- Write your function in a way that allows for the possibilty of any number of classes.\n",
    "- Assume that the document, `doc`, is represented as a dictionary that maps words (in the document) to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def classify(doc,priors,c_probs):\n",
    "\n",
    "    #<put your definition of classify here>\n",
    "   \n",
    "        \n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "sent = \"looking cloudy today\"\n",
    "doc = {(word, True) for word in sent.split()}\n",
    "classify(doc,c_priors,c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A problem\n",
    "There is a problem with the classifier that we have written. \n",
    "\n",
    "### Exercise 2.1\n",
    "* Can you find example sentences which give you different answers on different runs of the classifier\n",
    "* Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one smoothing\n",
    "It will often be the case that a word appears in documents of one class, but not in any documents of another class.\n",
    "- For example, consider the document \"city looking cloudy today\".\n",
    "- While \"city\" appears in documents of class `football`, it does not appear in any documents of class `weather`. \n",
    "- Thus, the conditional probabiity $P(\\,\\mbox{city}\\,|\\,\\mbox{weather}\\,)$ is equal to zero.\n",
    "- We are **multiplying** probabilities, so the document ends up with a score of zero even though all of the other words in the document suggest that it is of class `weather`.\n",
    "\n",
    "To get around this we need to do something called **smoothing** in order to avoid zero probabilities. \n",
    "\n",
    "In particular, we will implement a version of smoothing called **add-one smoothing**. This involves adding a count of one to all of the known vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known vocabulary\n",
    "The words that appear in documents in the training data are collectively described as the known vocabulary. These are the words that the classifier can learn something about. If the classifier is asked to classify a document that contains any words that are not in the known vocabulary then the classifier will simply ignore them.\n",
    "\n",
    "### Exercise 2.2\n",
    "In the cell below, write a function `known_vocabulary(training_data)` that takes some training and returns a set containing all of words that appear in documents in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing add-one smoothing\n",
    "As the name suggests, add-one smoothing involves adding counts.\n",
    "\n",
    "In particular, for each word, $w$, in the known vocabulary and each class, $c$, we add one extra count to our record of how many times $w$ appears in documents of class $c$. We are, in effect, hallucinating counts. The reason for doing this is that it means that we avoid zero probabilties.\n",
    "\n",
    "### Exercise 2.3\n",
    "In the blank cell below copy in your code for the `cond_probs` function that you wrote earlier. Then adapt this code so that it implements the add-one smoothing scheme.\n",
    "- You will find it useful to use your `known_vocabulary` function.\n",
    "- If there are $k$ words in your known vocabulary, then you will add $k$ counts for each class. \n",
    "- Therefore, when calculating conditional probabilities, you need to add $k$ to the denominator to account for these extra counts.\n",
    "\n",
    "Test out your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing class priors\n",
    "It is possible (though perhaps unlikely) that the training data does not contain any data for one class. In order to take care of this, we also smooth the class priors.\n",
    "\n",
    "### Exercise 2.4\n",
    "In the cell below, revise the `class_priors` function so that it adds one to the count of each class.\n",
    "\n",
    "Check that the sum of the priors for each of the classes is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignoring OOV words\n",
    "We now look at how to update the `classify` function that we wrote earlier so that it ignores out of vocabulary words that appear in a document being classified.\n",
    "\n",
    "### Exercise 2.5\n",
    "In the blank cell below, copy the `classify` method you wrote earlier and update it so that words not in the known vocabulary are ignored.\n",
    "- You will want to add an additional argument to the `classify` function that is a set containing the known vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underflow\n",
    "\n",
    "We need to address one final problem concerning the multiplication of probabilities.\n",
    "\n",
    "Recall this equation from earlier:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,d\\,|\\,c\\,) &=& P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,)\\\\\n",
    "&=& \\prod_{i=1}^n P(\\,w_i\\,|\\,c\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This tells us that in order to compute $P(\\,d\\,|\\,c\\,)$ for some document $d$ and class $c$, we must multiply $n$ conditional probabilities, one for each word in the document.\n",
    "\n",
    "While in our toy example, this is not an issue. However, in a more realistic settings, where we had thousands of documents, each of which contained multiple paragraphs, we would find ourselves multiplying large numbers of very small probabilities. This would lead to **underflow**.\n",
    "\n",
    "To avoid ths problem, we will add the log of probabilties. \n",
    "\n",
    "To understand why this is a reasonable thing to do let us recall a comparison from earlier:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)$$\n",
    "\n",
    "Our goal is to determine which of the values (on the left and right) is larger (or determine that they are equal). \n",
    "\n",
    "It should be clear that we will get exactly the same answer to this question by making the following comparsion.\n",
    "\n",
    "$$\\log(P(\\,d\\,|\\,\\mbox{weather}\\,)) + \\log(P(\\,\\mbox{weather}\\,)) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad \\log(P(\\,d\\,|\\,\\mbox{football}\\,)) + \\log(P(\\,\\mbox{football}\\,))$$\n",
    "\n",
    "Thus, rather than calculating conditional probabilities as described above, we will calculate log conditional probabilities like this:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log(P(\\,\\mbox{\"today is looking cloudy\"}\\,|\\,\\mbox{weather}\\,)) &=& \\log(P(\\{\\mbox{\"today\"},\\mbox{\"is\"},\\mbox{\"looking\"},\\mbox{\"cloudy\"}\\}\\,|\\,\\mbox{weather}\\,))\\\\\n",
    "&=& \\log(P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,))\\ +\\\\\n",
    "&&\\log(P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,))\\ +\\\\\n",
    "&&\\log(P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,))\\ + \\\\\n",
    "&&\\log(P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,))\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For the general case, with class $c$ and document $d=\\{w_1,\\ldots,w_n\\}$, we have:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log(P(\\,d\\,|\\,c\\,)) &=& \\log(P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,))\\\\\n",
    "&=& \\sum_{i=1}^n \\log(P(\\,w_i\\,|\\,c\\,))\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6\n",
    "In the blank cell below, make a copy of the cell containing the definition of `classify`.\n",
    "\n",
    "Adapt the code so that it adds logs of probabilties rather than multiplies probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a Naïve Bayes classifier\n",
    "\n",
    "We are now almost ready to run our Naïve Bayes classifier on a set of test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_sents_test = [\n",
    "    \"the weather today is nice\",\n",
    "    \"it is raining cats and dogs\",\n",
    "    \"the weather here is wet\",\n",
    "    \"it was hot today\",\n",
    "    \"rain due tomorrow\",\n",
    "]\n",
    "\n",
    "football_sents_test = [\n",
    "    \"what a great goal that was\",\n",
    "    \"poor defending by the city center back\",\n",
    "    \"wow he missed a sitter\",\n",
    "    \"united are a shambles\",\n",
    "    \"shots raining down on the keeper\",\n",
    "]\n",
    "\n",
    "weather_data_test = [({(word, True) for word in sent.split()}, \"weather\") for sent in weather_sents_test] \n",
    "football_data_test = [({(word, True) for word in sent.split()}, \"football\") for sent in football_sents_test]\n",
    "test_data = weather_data_test + football_data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.7\n",
    "* Complete the NBClassifier `class` which can be used to carry out classification.  Add your code (from earlier) to the methods `set_known_vocabulary`, `set_priors` and `set_cond_probs` and `classify`.  \n",
    "* Run your NB classifier on both the training data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._labels=[\"P\",\"N\"]\n",
    "        pass\n",
    "    \n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    \n",
    "    def _set_known_vocabulary(self,training_data):\n",
    "        #add your code here to set self.known\n",
    "       \n",
    "    \n",
    "    def _set_priors(self,training_data):\n",
    "        #add your code here to set self.priors\n",
    "      \n",
    "    def _set_cond_probs(self,training_data):       \n",
    "        #add your code here to set self.conds\n",
    "        \n",
    "    \n",
    "    def train(self,training_data):\n",
    "        self._set_known_vocabulary(training_data)\n",
    "        self._set_priors(training_data)\n",
    "        self._set_cond_probs(training_data)\n",
    "    \n",
    "    def classify(self,doc):\n",
    "        #add your code here\n",
    "        \n",
    "    #the parent class classifierI has a classifier_many class which applies self.classify to every item in a list\n",
    "    #so it does not need to be defined here.\n",
    "    #https://www.nltk.org/_modules/nltk/classify/api.html\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of an NBClassifier, train it and call self.classify_many on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
