{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Further Document Classification (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This topic builds on the activities of the previous topic on sentiment analysis. You will be focussing on the Amazon review corpus with a view to investigating the following issues.\n",
    "\n",
    "- Evaluation metrics for classifier performance\n",
    "- What is the impact of varying training data size? To what extent does increasing the quantity of training data improve classifier performance?\n",
    "- What is the impact of changing domain (i.e. book, dvd, electronics, kitchen). In particular, what happens if you train a classifier with reviews in one domain (or product category) and test the classifier on reviews from a different domain? Does performance degrade, and if so, by how much? Are some pairs of product categories more similar than others?\n",
    "- What is the impact on classifier accuracy of various feature extraction methods?\n",
    "\n",
    "By this stage, you should be very comfortable with Python's [list comprehensions](http://docs.python.org/tutorial/datastructures.html#list-comprehensions) and [slice](http://bergbom.blogspot.co.uk/2011/04/python-slice-notation.html) notation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To access functionality defined in previous notebooks, copy the functions defined in Week3Labs into a `week3.py` file and then import it into the notebook.  There is a `week3.py` file included with these resources which you can update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex NLTK root directory is /Users/juliewe/Documents/teaching/NLE2018/resources\n"
     ]
    }
   ],
   "source": [
    "#import code to setup training and testing data, wordlist classifiers and NB classifiers\n",
    "\n",
    "#from week3 import *\n",
    "from week3complete import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ipython magic commands and any other imports here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Classifier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Here is code that imports an evaluation function <code>evaluate_wordlist_classifier</code> which can be used to determine how well a word_list classifier performs. This function returns the <b>accuracy</b> of a classifier. The accuracy metric is defined as the proportion of documents that were correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.stats import evaluate_wordlist_classifier\n",
    "\n",
    "#Create a new classifier\n",
    "#Make sure you have updated the code in week3.py to contain your WordList Classifier\n",
    "top_pos=[]\n",
    "top_neg=[]\n",
    "book_classifier = SimpleClassifier(top_pos, top_neg)\n",
    "\n",
    "#Evaluate classifier\n",
    "#The function requires three arguments:\n",
    "# 1. Word list based classifer\n",
    "# 2. A list (or generator) of positive AmazonReview objects\n",
    "# 3. A list (or generator) of negative AmazonReview objects\n",
    "score = evaluate_wordlist_classifier(book_classifier, pos_test, neg_test)  \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have run the cell above without updating the SimpleClassifier code you should see that the accuracy is 0.5 i.e., 50%. The original SimpleClassifier just assigns everything to the positive class.  Since it is a binary classification decision and the classes are balanced, it will get 50% of the decisions correct (those that are positive) and 50% of the decisions incorrect (those that are actually negative).  This is the **baseline** result for this kind of classification task.  We obviously want to build classifiers that do better than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Naïve Bayes classifier on test data\n",
    "We are now ready to run our Naïve Bayes classifier on a set of test data. When we do this we want to return the accuracy of the classifier on that data, where accuracy is calculated as follows:\n",
    "\n",
    "$$\\frac{\\mbox{number of test documents that the classifier classifiers correctly}}\n",
    "{\\mbox{total number of test documents}}$$\n",
    "\n",
    "In order to compute this accuracy score, we need to give the classifier **labelled** test data.\n",
    "- This will be in the same format as the training data.\n",
    "\n",
    ">In the cell below, we set up 5 test documents in the class `weather` and 5 documents in the class `football`.\n",
    "\n",
    ">Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "]\n",
    "\n",
    "weather_data_train = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_train] \n",
    "football_data_train = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_train]\n",
    "train_data = weather_data_train + football_data_train\n",
    "\n",
    "weather_sents_test = [\n",
    "    \"the weather today is nice\",\n",
    "    \"it is raining cats and dogs\",\n",
    "    \"the weather here is wet\",\n",
    "    \"it was hot today\",\n",
    "    \"rain due tomorrow\",\n",
    "]\n",
    "\n",
    "football_sents_test = [\n",
    "    \"what a great goal that was\",\n",
    "    \"poor defending by the city center back\",\n",
    "    \"wow he missed a sitter\",\n",
    "    \"united are a shambles\",\n",
    "    \"shots raining down on the keeper\",\n",
    "]\n",
    "\n",
    "weather_data_test = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_test] \n",
    "football_data_test = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_test]\n",
    "test_data = weather_data_test + football_data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'is': True, 'it': True, 'raining': True, 'today': True}, 'weather'),\n",
       " ({'cloudy': True, 'looking': True, 'today': True}, 'weather'),\n",
       " ({'is': True, 'it': True, 'nice': True, 'weather': True}, 'weather'),\n",
       " ({'city': True, 'good': True, 'looking': True}, 'football'),\n",
       " ({'advantage': True, 'united': True}, 'football')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'is': True, 'nice': True, 'the': True, 'today': True, 'weather': True},\n",
       "  'weather'),\n",
       " ({'and': True,\n",
       "   'cats': True,\n",
       "   'dogs': True,\n",
       "   'is': True,\n",
       "   'it': True,\n",
       "   'raining': True},\n",
       "  'weather'),\n",
       " ({'here': True, 'is': True, 'the': True, 'weather': True, 'wet': True},\n",
       "  'weather'),\n",
       " ({'hot': True, 'it': True, 'today': True, 'was': True}, 'weather'),\n",
       " ({'due': True, 'rain': True, 'tomorrow': True}, 'weather'),\n",
       " ({'a': True,\n",
       "   'goal': True,\n",
       "   'great': True,\n",
       "   'that': True,\n",
       "   'was': True,\n",
       "   'what': True},\n",
       "  'football'),\n",
       " ({'back': True,\n",
       "   'by': True,\n",
       "   'center': True,\n",
       "   'city': True,\n",
       "   'defending': True,\n",
       "   'poor': True,\n",
       "   'the': True},\n",
       "  'football'),\n",
       " ({'a': True, 'he': True, 'missed': True, 'sitter': True, 'wow': True},\n",
       "  'football'),\n",
       " ({'a': True, 'are': True, 'shambles': True, 'united': True}, 'football'),\n",
       " ({'down': True,\n",
       "   'keeper': True,\n",
       "   'on': True,\n",
       "   'raining': True,\n",
       "   'shots': True,\n",
       "   'the': True},\n",
       "  'football')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "In the cell below implement a `NB_evaluate` function that returns the accuracy of a classifier on a set of labelled test data.\n",
    "`NB_evaluate` should take the following arguments:\n",
    "- a (trained) NBClassifier\n",
    "- the labelled test data\n",
    "\n",
    "If you have not implemented your own NBClassifier as a stand-alone class, you could implement a version of the `NB_evaluate` function which makes use of the `classify` function, and take the following arguments:\n",
    "- the test data\n",
    "- the class priors\n",
    "- the conditional probabilities\n",
    "- the known vocabulary (though this is redundant since it could be computed from the conditional probabilities)\n",
    "\n",
    "In any case, `NB_evaluate` should return the accuracy of the classifier on the test data.\n",
    "\n",
    "Try out your `NB_evaluate` function on the test data in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_evaluate(nb_classifier,test_data):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall and F1 score etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the impact of the quantity of training data\n",
    "We will begin by exploring the impact on classification accuracy of using different quantities of training data.\n",
    "\n",
    "We have assembled code from the notebook for Topic 2 in a module called classification_utils.  In the next cell, we load this module.\n",
    "\n",
    "We now measure the performance of both the word list classifier (the version that uses the 100 most frequent words in each category) and Naïve Bayes classifiers on all of the dvd reviews in the extended dvd review corpus, with 70% of the corpus being used for training and the remainder for testing.\n",
    "\n",
    "### Exercise\n",
    "Run this cell several times and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_utils import *\n",
    "\n",
    "reader = AmazonReviewCorpusReader().category(\"dvd\")\n",
    "#stopwords = stopwords.words('english')\n",
    "word_list_size = 100\n",
    "pos_train,neg_train,pos_test,neg_test = get_train_test_data(reader)\n",
    "WL_accuracy = run_WL(pos_train,neg_train,pos_test,neg_test,word_list_size)\n",
    "NB_accuracy = run_NB(pos_train,neg_train,pos_test,neg_test)\n",
    "print(\"The accuracy of Word List classifer is {0:.2f}\".format(WL_accuracy))\n",
    "print(\"The accuracy of the Naive Bayes classifier is {0:.2f}.\".format(NB_accuracy))\n",
    "df = pd.DataFrame([(\"Word List\",WL_accuracy),(\"NB\",NB_accuracy)])\n",
    "display(df)\n",
    "ax = df.plot.bar(title=\"Experimental Results\",legend=False,x=0)\n",
    "ax.set_ylabel(\"Classifier Accuracy\")\n",
    "ax.set_xlabel(\"Classifier\")\n",
    "ax.set_ylim(0.5,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classifiers have different accuracies on different runs. \n",
    "\n",
    "### Exercise\n",
    "Copy the cell above and move the copy to be positioned below this cell. Then adapt the code so that the accuracy reported for each classifier is the average across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/average_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The next step involves measuring the performance of both the word list and Naïve Bayes classifiers on a range of subsets of the dvd reviews in the extended dvd review corpus.\n",
    "\n",
    "- The full data set has 1000 positive and 1000 negative reviews. \n",
    "- You should continue to use 30% of the data for testing, so this means that we have up to 700 positive and 700 negative reviews to sample from.\n",
    "- Consider (at least) the following sample sizes: 1, 10, 50, 100, 200, 400, 600 and 700.\n",
    "- Note that the sample size is not the total number of reviews, but the number of positive reviews (which is also equal to the number of negative reviews).\n",
    "\n",
    "### Exercise\n",
    "Copy the code cell that you created for the last exercise, and place the copy below this cell. Then adapt the code to determine the accurracy of each classifier on each subsets.\n",
    "\n",
    "Use the `sample` function from the random module, which means you should include the line:  \n",
    "`from random import sample`\n",
    "- Make sure that you are selecting samples that have an equal number of positive and negative reviews.\n",
    "\n",
    "Use a Pandas dataframe to display the results in a table.\n",
    "- The table should have three columns:\n",
    " - the first for the sample sizes, \n",
    " - the second for the Word List classifier accuracies, and \n",
    " - the third for the Naïve Bayes classifier accuracies.\n",
    "- There are examples of this in the model solutions to exercises in Topic 1 that you can adapt.\n",
    "- You can use `pd.set_option('precision',2)` to limit the reals to have 2 digits after the decimal point.\n",
    "- Create a dataframe like this:\n",
    "```\n",
    "pd.DataFrame(list(zip(<column 1 list>, <column 2 list>, ...)),\n",
    "                  columns=<a list of the column headings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/different_sample_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Make a copy of the cell you created for the previous exercise and move it to be positioned below this cell. Using the new cell, repeat the above for each of the product categories.\n",
    "- The available categories are `'dvd'`, `'book'`, `'kitchen'` and `'electronics'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/different_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-domain sentiment analysis\n",
    "We now consider the extend to which the performance of a Naïve Bayes classifier is degraded due to differences between the data it is trained on and the data that it is tested on. \n",
    "\n",
    "For example, suppose we train a classifier on book reviews and then test that classifier on a collection of dvd reviews. Does it perform as well as it would when trained on dvd reviews?\n",
    "\n",
    "We will refer to the domain or product category that the classifier is trained on as the **source domain** and the domain or product category that the classifier is tested on as the **target domain**. You will be experimenting with different combinations of source and target domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 product categories so there are 16 different ways in which these can be combined to create training and testing datasets.\n",
    "\n",
    "### Exercise\n",
    "In the empty cell below, write code that determines the accuracy of the Naïve Bayes classifier for each of these 16 combinations.\n",
    "- use a pandas dataframe to report the results in a table with three columns:\n",
    " - the first column is the source category,\n",
    " - the second column is the target category, and\n",
    " - the third column is the accuracy for the corresponing source and target categories.\n",
    "- So each row of the table gives the accuracy for one of the combinations.\n",
    " - There should, therefore, be 16 rows.\n",
    "\n",
    "Ideally, the accuracies that you report should be averaged over multiple runs (as we saw above). However, since there are 16 combinations to consider, in order to avoid overly long running times, you do not need to run the classifier more than once for each combination.\n",
    "\n",
    "Now that we are just running the Naïve Bayes classifier, it makes sense to format the data before passing it to our `run_NB` function. \n",
    "\n",
    "We having included in `classification_util.py` a variant of `get_train_test_data` called `get_formatted_train_test_data` that is defined as follows:\n",
    "\n",
    "```\n",
    "def get_formatted_train_test_data(category, feature_extractor=None, split=0.7):\n",
    "    '''\n",
    "    Helper function. Splits data evenly across positive and negative, and then formats it\n",
    "    ready for naive bayes. You can also optionally pass in your custom feature extractor \n",
    "    (see next section), and a custom split ratio.\n",
    "    '''\n",
    "    arcr = AmazonReviewCorpusReader()\n",
    "    pos_train, pos_test = split_data(arcr.positive().category(category).documents(), split)\n",
    "    neg_train, neg_test = split_data(arcr.negative().category(category).documents(), split)\n",
    "    train = format_data(pos_train, \"pos\", feature_extractor) + format_data(neg_train, \"neg\", feature_extractor)\n",
    "    test  = format_data(pos_test, \"pos\", feature_extractor) + format_data(neg_test, \"neg\", feature_extractor)\n",
    "    return test, train\n",
    "```\n",
    "\n",
    "This means that we also have a variant of `run_NB` called `run_NB_preformatted` which is defined as follows:\n",
    "\n",
    "```\n",
    "def run_NB_preformatted(train,test):\n",
    "    c_priors = class_priors(train)\n",
    "    c_probs = cond_probs(train)\n",
    "    known_vocab = known_vocabulary(train)\n",
    "    return NB_evaluate(test,c_priors,c_probs,known_vocab)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/crossing_domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Make a copy of the cell that contains your solution to the previous exercise and position the copy below this cell.\n",
    "\n",
    "Adapt the code so that you explore the use of training sets built from multiple categories. For example, you might consider the following:\n",
    "\n",
    "```\n",
    "source = dvd_train + book_train + kitchen_train\n",
    "target = electronics_test\n",
    "\n",
    "source = dvd_train + book_train + kitchen_train + electronics_train\n",
    "target = electronics_test\n",
    "```\n",
    "\n",
    "One thing to bear in mind when considering the impact of using multiple product categories is the extent to which improvements are due to an increase in the quanty of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/multi_category_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "So far, the Naïve Bayes classifiers you've been training, have been using all of the tokens in a review as features. You will now be exploring whether it is possible to improve classification accuracy by extracting different features from the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "First, establish the accuracy of the Naïve Bayes classifier on the each of the product categories.\n",
    "\n",
    "To do this, simply run the following cell. This code creates a dictionary `baseline`  that stores the accuracy for each product category. You can use this dictionary later when considering the impact of various feature extractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from classification_utils import *\n",
    "\n",
    "prod_cats = [\"book\",\"dvd\",\"kitchen\",\"electronics\"]\n",
    "baseline = {}\n",
    "for prod_cat in prod_cats:\n",
    "    repetitions = 2 # accuracy figures are averaged over this many repetitions\n",
    "    NB_accuracy_tot = 0\n",
    "    for i in range(repetitions): # for each sample_size we will find average accuracy over several repetitions\n",
    "        test, train   = get_formatted_train_test_data(prod_cat)\n",
    "        NB_accuracy_tot += run_NB_preformatted(train,test)\n",
    "    baseline[prod_cat] = NB_accuracy_tot/repetitions\n",
    "    \n",
    "pd.set_option('precision',2)\n",
    "df = pd.DataFrame.from_dict(baseline,orient='index')\n",
    "display(df)\n",
    "ax = df.plot.bar(title=\"Experimental Results\",legend=False)\n",
    "ax.set_ylabel(\"Classifier Accuracy\")\n",
    "ax.set_xlabel(\"Product Category\")\n",
    "ax.set_ylim(0.5,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the empty cell below, define the feature extractor function `FE_all`, which takes as input an `AmazonReview` object, and outputs a list of strings, where each string is a feature to be used by the Naïve Bayes classifier. \n",
    "\n",
    "Initially, define it to return all of the tokens in the review. We will adapt it in the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/FE_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "Now that you have defined a feature extractor function, you can pass it to the helper function `get_training_testing` that was used in the last exercise. \n",
    "\n",
    "Copy the code cell above that you used to produce the `baseline` dictionary, and adapt it to make use of your feature_extractor function, `FE_all`. \n",
    "\n",
    "Run the new code cell, and save the results in a new dictionary called `FE_all_results`. This feature extractor is have no impact on the features that the classifier is using so should not have a significant impact on the accuracy of the classifier.  Check that this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/test_FE_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In a new cell, define a feature extraction function that converts all tokens to lowercase. \n",
    "- Call your new feature extractor `FE_lower`.\n",
    "- See the last section of the Topic 1 notebook for guidance on how to convert tokens to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/FE_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In a new cell, define a feature extraction function that converts all numbers to \"NUM\". \n",
    "- Call your new feature extractor `FE_NUM`.\n",
    "- See the last section of the Topic 1 notebook for guidance on how to convert tokens to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/FE_NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In a new cell, define a feature extraction function that filters out non-alphabetic words and stopwords.\n",
    "- Call your new feature extractor `FE_puncstop`.\n",
    "- See the last section of the Topic 1 notebook for guidance on how to convert tokens to lowercase.\n",
    "- Note that the following two lines must be placed in a cell where this feature extraction function is being used:  \n",
    "`from nltk.corpus import stopwords`  \n",
    "`stopwords = stopwords.words('english')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/FE_puncstop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In a new cell, define a feature extraction function that stems all of the tokens.\n",
    "- Call your new feature extractor `FE_stem`.\n",
    "- The code snippet below shows you how to set up a stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer() #Create a new stemmer\n",
    "stemmed = stemmer.stem(\"complications\") #Example usage, stemming a single word\n",
    "\n",
    "#You will need to stem all of the words in a review, \n",
    "#this will require iterating over them with a loop or list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/FE_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Now that you have defined several feature extraction functions, it is time to look at their impact on performance.\n",
    "\n",
    "Make a copy of the cell that you used to determine the accuracy of your first feature extractor, and position the copied cell below this one.\n",
    "\n",
    "Extend the code in the cell to do the following:\n",
    "- Create a dictionary for each of your feature extraction functions.\n",
    "- Display the results in a table using a Pandas dataframe. This table should have one row for each product category and one column for the product category names and additional columns for each of the five feature extraction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/test_FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
