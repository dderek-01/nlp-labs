{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 (Part 2): Document Similarity\n",
    "\n",
    "In some applications, it may be difficult to define the classes that we want to use in classification ahead of time.  Or, classes might be made up various subclasses (which differ in terms of the vocabulary used).  In both of these cases (and others), it might be more appropriate to think about **document similarity**.  For a new document, can we find the most similar document in our collection?\n",
    "\n",
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/juliewe/resources')\n",
    "\n",
    "from utils import * #utils has functions that we have worked on in previous weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get a document collection.  Actually, lets get two collections and store them in a dictionary for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "\n",
    "rcr = ReutersCorpusReader()    #Create a new reader\n",
    "\n",
    "collectionsize=10\n",
    "collections={\"finance\":[],\"sport\":[]}\n",
    "\n",
    "for key in collections.keys():\n",
    "    generator=rcr.category(key).raw()\n",
    "    while len(collections[key])<collectionsize:\n",
    "        collections[key].append(next(generator))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need a function which will tokenise documents and construct a *bag-of-words* document representation.  Combining some of the functionality we have been working over the past few weeks (which we have imported from utils.py), we could use something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow(somestring):\n",
    "    rep=word_tokenize(somestring)\n",
    "    rep=normalise(rep)\n",
    "    rep=stem(rep)\n",
    "    rep=filter_stopwords(rep)\n",
    "    dict_rep={}\n",
    "    for token in rep:\n",
    "        dict_rep[token]=dict_rep.get(token,0)+1\n",
    "    return(dict_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply `make_bow()` to all of the documents in our collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_collections={key:[make_bow(doc) for doc in collection] for key,collection in collections.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bow_collections[\"finance\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "We are going to use the cosine measure to determine how similar two documents are.  This can be defined in terms of the dot products of vectors:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mbox{sim}_{\\mbox{cosine}}(A,B) = \\frac{A.B}{\\sqrt{A.A \\times B.B}}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where the dot product of two vectors, A and B, is defined as:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "A.B = \\sum_{\\mbox{f}} \\mbox{weight}(A,f)\\times \\mbox{weight}(B,f) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "and $\\mbox{weight}(X,f)$ tells us the value associated with feature $f$ in the vector representation of $X$\n",
    "\n",
    "### Exercise 1.1\n",
    "* Write a function `dot` which takes two documents (represented as dictionaries) and returns their dot product\n",
    "* Test it out on the first two documents in the finance collection.  You should get the answer 206 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "* Write a function `cos_sim` which takes two documents (represented as dictionaries) and returns their cosine similarity.\n",
    "* Your function should make 3 calls to the `dot` function you have already defined\n",
    "* If you test it out on the first two documents in the finance collection you should get 0.24 (to 2S.F.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "* Write some code that will compute the similarity of every document in a collection with every document in another collection\n",
    "* Write code to compute the average similarity of two collections\n",
    "* Compute (and display) the average similarity of \n",
    "    * the finance collection to the finance collection\n",
    "    * the sport collection to the sport collection\n",
    "    * the finance collection to the sport collection\n",
    "    * the sport collection to the finance collection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Frequency\n",
    "Frequency of a word in a document does not make a very good weight because some words occur very frequently in all documents.  If two rare words occur in both of our pair of documents, that should add more to their perceived similarity than if two common words occur in both of our pair of documents.\n",
    "\n",
    "### TF-IDF\n",
    "A commonly used weight is tf-idf which stands for **term frequency, inverse document frequency**\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mbox{tf-idf}(D_i,f) = tf(D_i,f) \\times idf(D_i,f)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $tf(D_i,f)$ is simply the frequency of feature f in document $D_i$\n",
    "and\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "idf(D_i,f) = log \\frac{N}{df(f)}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $N$ is the total number of documents and $\\mbox{df}(f)$ is the number of documents containing $f$:  \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "df(f)=|\\{i|\\mbox{freq}(D_i,f)>0\\}|\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The code below will take a list of documents (represented as dictionaries) and compute the document frequency for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(doclist):\n",
    "    df={}\n",
    "    for doc in doclist:\n",
    "        for feat in doc.keys():\n",
    "            df[feat]=df.get(feat,0)+1\n",
    "            \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "* Write a function which will compute the idf values for features given a list of documents\n",
    "* Use it to compute idf values for features given:\n",
    "    * the finance collection of documents\n",
    "    * the sports collection of documents\n",
    "    * the combination of the finance and sports collections\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "* Write a function `convert_to_tfidf` that takes two arguments:\n",
    "    * a list of documents (represented as dictionaries {feat:freq})\n",
    "    * a dictionary containing idf values\n",
    "* and outputs a list of documents with tfidf weights (i.e., dictionaries {feat:tfidf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_to_tfidf(bow_collections['finance'],finance_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "* Convert both of your document collections so that the weights are tfidf values rather than frequencies.  Which idf_values should you use in each case?\n",
    "* Recompute the average similarity between each collection of documents (as in Ex 1.3).\n",
    "* What do you notice?\n",
    "* As an extension, try increasing the sizes of the collections.  What do you notice now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
