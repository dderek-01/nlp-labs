{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Further Document Classification (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This topic builds on the activities of the previous topic on sentiment analysis. You will be focussing on the Amazon review corpus with a view to investigating the following issues.\n",
    "\n",
    "- Evaluation metrics for classifier performance\n",
    "- What is the impact of varying training data size? To what extent does increasing the quantity of training data improve classifier performance?\n",
    "- What is the impact of changing domain (i.e. book, dvd, electronics, kitchen). \n",
    "\n",
    "By this stage, you should be very comfortable with Python's [list comprehensions](http://docs.python.org/tutorial/datastructures.html#list-comprehensions) and [slice](http://bergbom.blogspot.co.uk/2011/04/python-slice-notation.html) notation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To access functionality defined in previous notebooks, copy the functions defined in Week3Labs into a `week3.py` file and then import it into the notebook.  There is a `week3.py` file included with these resources which you can update.  Alternatively, if you haven't been able to complete week 3, there is a `week3complete.py` file included to help you make progress this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import code to setup training and testing data, wordlist classifiers and NB classifiers\n",
    "\n",
    "from week3 import *\n",
    "#from week3complete import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ipython magic commands and any other imports here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Classifier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Here is code that imports an evaluation function <code>evaluate_wordlist_classifier</code> which can be used to determine how well a word_list classifier performs. This function returns the <b>accuracy</b> of a classifier. The accuracy metric is defined as the proportion of documents that were correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sussex_nltk.stats import evaluate_wordlist_classifier\n",
    "\n",
    "#Create a new classifier\n",
    "#Make sure you have updated the code in week3.py to contain your WordList Classifier\n",
    "top_pos=[]\n",
    "top_neg=[]\n",
    "#dvd_classifier = SimpleClassifier(top_pos, top_neg)\n",
    "dvd_classifier = SimpleClassifier_mf(100)\n",
    "dvd_classifier.train(pos_train,neg_train)\n",
    "\n",
    "#Evaluate classifier\n",
    "#The function requires three arguments:\n",
    "# 1. Word list based classifer\n",
    "# 2. A list (or generator) of positive AmazonReview objects\n",
    "# 3. A list (or generator) of negative AmazonReview objects\n",
    "score = evaluate_wordlist_classifier(dvd_classifier, pos_test, neg_test)  \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have run the cell above without updating the SimpleClassifier code you should see that the accuracy is 0.5 i.e., 50%. The original SimpleClassifier just assigns everything to the positive class.  Since it is a binary classification decision and the classes are balanced, it will get 50% of the decisions correct (those that are positive) and 50% of the decisions incorrect (those that are actually negative).  This is the **baseline** result for this kind of classification task.  We obviously want to build classifiers that do better than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Naïve Bayes classifier on test data\n",
    "We are now ready to run our Naïve Bayes classifier on a set of test data. When we do this we want to return the accuracy of the classifier on that data, where accuracy is calculated as follows:\n",
    "\n",
    "$$\\frac{\\mbox{number of test documents that the classifier classifiers correctly}}\n",
    "{\\mbox{total number of test documents}}$$\n",
    "\n",
    "In order to compute this accuracy score, we need to give the classifier **labelled** test data.\n",
    "- This will be in the same format as the training data.\n",
    "\n",
    ">In the cell below, we set up 5 test documents in the class `weather` and 5 documents in the class `football`.\n",
    "\n",
    ">Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "]\n",
    "\n",
    "weather_data_train = [({word: True for word in sent.split()}, \"weather\") for sent in weather_sents_train] \n",
    "football_data_train = [({word: True for word in sent.split()}, \"football\") for sent in football_sents_train]\n",
    "train_data = weather_data_train + football_data_train\n",
    "\n",
    "weather_sents_test = [\n",
    "    \"the weather today is nice\",\n",
    "    \"it is raining cats and dogs\",\n",
    "    \"the weather here is wet\",\n",
    "    \"it was hot today\",\n",
    "    \"rain due tomorrow\",\n",
    "]\n",
    "\n",
    "football_sents_test = [\n",
    "    \"what a great goal that was\",\n",
    "    \"poor defending by the city center back\",\n",
    "    \"wow he missed a sitter\",\n",
    "    \"united are a shambles\",\n",
    "    \"shots raining down on the keeper\",\n",
    "]\n",
    "\n",
    "weather_data_test = [({word: True for word in sent.split()}, \"weather\") for sent in weather_sents_test] \n",
    "football_data_test = [({word: True for word in sent.split()}, \"football\") for sent in football_sents_test]\n",
    "test_data = weather_data_test + football_data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "In the cell below implement a `classifier_evaluate` function that returns the accuracy of a classifier on a set of labelled test data.\n",
    "`classifier_evaluate` should take the following arguments:\n",
    "- a (trained) classifier (e.g., an instance of NBClassifier)\n",
    "- the labelled test data\n",
    "\n",
    "If you have not implemented your own NBClassifier as a stand-alone class, you could implement a version of the `classifier_evaluate` function which makes use of the `classify` function, and take the following arguments:\n",
    "- the test data\n",
    "- the class priors\n",
    "- the conditional probabilities\n",
    "- the known vocabulary (though this is redundant since it could be computed from the conditional probabilities)\n",
    "\n",
    "In any case, `classifier_evaluate` should return the accuracy of the classifier on the test data.\n",
    "\n",
    "Try out your `classifier_evaluate` function on the test data in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "If you have written your classifier_evaluate() code in a fairly generic way, you should find that it is **not** specific to NB classification.  You should be able to pass it any classifier and test_data (formatted in the same way) and evaluate the accuracy.  \n",
    "* Format the test_data for the Amazon reviews in the same way as the weather_football sentences (i.e., convert the list of documents into a list of (document,label pairs)\n",
    "* Make any updates necessary to your classifier_evaluate() code\n",
    "* Use your function to evaluate the accuracy of the SimpleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "Now, we want to run your NB classifier on a real problem - the classification of Amazon reviews as positive or negative.\n",
    "* use your feature extraction code from Lab_3_2 to convert the Amazon Review corpus training data into the same format that your NB_classifier expects.\n",
    "* train a nb_classifier on this training data\n",
    "* test it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and F1 score etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When classes are unbalanced, evaluating classifiers in terms of accuracy can be misleading.  For example, if 10% of documents are relevant and 90% of documents are irrelevant, then a classifier which labels all documents as irrelevant will obtain an accuracy of 90%.  This sounds good but is actually useless. More useful metrics for evaluation of performance are precision, recall and F1 score.  These metrics allow us to distinguish the different types of errors our classifiers make.\n",
    "\n",
    "For each class, $c$, we need to keep a record of \n",
    "* True Positives: $TP=|\\{i|\\mbox{prediction}(i)=\\mbox{label}(i)=c\\}|$\n",
    "* False Negatives: $FN=|\\{i|\\mbox{prediction}(i)\\neq \\mbox{label}(i)=c\\}|$\n",
    "* False Positives: $FP=|\\{i|\\mbox{label}(i) \\neq \\mbox{prediction}(i)=c\\}|$\n",
    "* True Negatives: $TN=|\\{i|\\mbox{prediction}(i)=\\mbox{label}(i)\\neq c\\}|$\n",
    "\n",
    "Note the symmetry in the binary classification task (the TN for one class are the TP for the other class and so on).  Therefore, in binary classification, we just record these values and compute the following evaluation metrics for a single class (e.g. \"Relevant\" or \"Positive\")\n",
    "\n",
    "* Precision: \n",
    "\\begin{eqnarray*}\n",
    "P=\\frac{TP}{TP+FP}\n",
    "\\end{eqnarray*}\n",
    "* Recall: \n",
    "\\begin{eqnarray*}\n",
    "R=\\frac{TP}{TP+FN}\n",
    "\\end{eqnarray*}\n",
    "* F1-score: \n",
    "\\begin{eqnarray*}\n",
    "F1 = \\frac{2\\times P\\times R}{P+R}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 2.1\n",
    " \n",
    " The code below defines a ConfusionMatrix class for the binary classification task.  Currently, it will compute the number of TPs, FPs, FNs and TNs.  Test it out with predictions and test data for \n",
    " * sentiment analysis task (Amazon book review data)\n",
    " * topic classification task (weather_football sentence data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    def __init__(self,predictions,goldstandard,classes=(\"P\",\"N\")):\n",
    "        (self.c1,self.c2)=classes\n",
    "        self.TP=0\n",
    "        self.FP=0\n",
    "        self.FN=0\n",
    "        self.TN=0\n",
    "        for p,g in zip(predictions,goldstandard):\n",
    "            if g==self.c1:\n",
    "                if p==self.c1:\n",
    "                    self.TP+=1\n",
    "                else:\n",
    "                    self.FN+=1\n",
    "            \n",
    "            elif p==self.c1:\n",
    "                self.FP+=1\n",
    "            else:\n",
    "                self.TN+=1\n",
    "        \n",
    "    \n",
    "    def precision(self):\n",
    "        p=0\n",
    "        #put your code to compute precision here\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def recall(self):\n",
    "        r=0\n",
    "        #put your code to compute recall here\n",
    "        \n",
    "        return r\n",
    "    \n",
    "    def f1(self):\n",
    "        f1=0\n",
    "        #put your code to compute f1 here\n",
    "         \n",
    "        return f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs will contain the documents to classify, labels contains the corresponding gold standard labels\n",
    "docs,labels=zip(*Amazon_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "* Add functionality to the ConfusionMatrix class code to compute precision, recall and F1 score\n",
    "* Use your code to evaluate the performance of the different classifiers you have constructed.\n",
    "* Interpret your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the impact of the quantity of training data\n",
    "We will begin by exploring the impact on classification accuracy of using different quantities of training data.\n",
    "\n",
    "The code in the cell below combines functionality built up earlier and will enable you to get training and testing data (in the correct format) for your classifiers.  It also defines a WordListClassifier class which expects training data in the same format as the NB Classifier - this is very important if we want to be able to easily switch between using different classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(review):\n",
    "    #print(review.words())\n",
    "    return {word:True for word in review.words()}\n",
    "\n",
    "def get_training_test_data(category):\n",
    "    reader=AmazonReviewCorpusReader().category(category)\n",
    "    pos_train, pos_test = split_data(reader.positive().documents())\n",
    "    neg_train, neg_test = split_data(reader.negative().documents())\n",
    "    train_data=[(feature_extract(review),'P')for review in pos_train]+[(feature_extract(review),'N') for review in neg_train]\n",
    "    test_data=[(feature_extract(review),'P')for review in pos_test]+[(feature_extract(review),'N') for review in neg_test]\n",
    "    return train_data,test_data\n",
    "\n",
    "\n",
    "\n",
    "class WordListClassifier(SimpleClassifier):\n",
    "    #this WordListClassifier uses the same feature representation as the NB classifier\n",
    "    #i.e., a multivariate Bernouilli event model where multiple occurrences of the same word in the same document are not counted.\n",
    "        \n",
    "    def __init__(self,k):\n",
    "        self._labels=[\"P\",\"N\"]\n",
    "        self.k=k\n",
    "        \n",
    "    def get_all_words(self,docs):\n",
    "        return reduce(lambda words,doc: words + list(doc.keys()), docs, [])\n",
    "    \n",
    "    def train(self,training_data):\n",
    "        pos_train=[doc for (doc,label) in training_data if label == self.labels()[0]]\n",
    "        neg_train=[doc for (doc,label) in training_data if label == self.labels()[1]]\n",
    "        \n",
    "        pos_freqdist=FreqDist(self.get_all_words(pos_train))\n",
    "        neg_freqdist=FreqDist(self.get_all_words(neg_train))\n",
    "        \n",
    "        self._pos=most_frequent_words(pos_freqdist,self.k)\n",
    "        self._neg=most_frequent_words(neg_freqdist,self.k)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the code in the cell below several times.  Each time it should generate a new sample of review data, train the classifiers and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training,testing=get_training_test_data(\"dvd\")\n",
    "\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "word_list_size = 100\n",
    "classifiers={\"Word List\":WordListClassifier(word_list_size),\n",
    "             \"Naive Bayes\":NBClassifier()}\n",
    "use=[\"Word List\",\"Naive Bayes\"]\n",
    "\n",
    "results=[]\n",
    "for name,classifier in classifiers.items():\n",
    "    if name in use:\n",
    "        classifier.train(training)\n",
    "        accuracy=classifier_evaluate(classifier,testing)\n",
    "        print(\"The accuracy of {} classifier is {}\".format(name,accuracy))\n",
    "        results.append((name,accuracy))\n",
    "             \n",
    "df = pd.DataFrame(results)\n",
    "display(df)\n",
    "ax = df.plot.bar(title=\"Experimental Results\",legend=False,x=0)\n",
    "ax.set_ylabel(\"Classifier Accuracy\")\n",
    "ax.set_xlabel(\"Classifier\")\n",
    "ax.set_ylim(0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classifiers have different accuracies on different runs. \n",
    "\n",
    "### Exercise 3.1\n",
    "Copy the cell above and move the copy to be positioned below this cell. Then adapt the code so that the accuracy reported for each classifier is the average across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "Adapt the code so that it calculates average precision, recall and F1-score rather than average accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step involves measuring the performance of both the word list and Naïve Bayes classifiers on a range of subsets of the dvd reviews in the extended dvd review corpus.\n",
    "\n",
    "- The full data set has 1000 positive and 1000 negative reviews. \n",
    "- You should continue to use 30% of the data for testing, so this means that we have up to 700 positive and 700 negative reviews to sample from.\n",
    "- Consider (at least) the following sample sizes: 1, 10, 50, 100, 200, 400, 600 and 700.\n",
    "- Note that the sample size is not the total number of reviews, but the number of positive reviews (which is also equal to the number of negative reviews).\n",
    "\n",
    "### Exercise 3.3\n",
    "Copy the code cell that you created for the last exercise, and place the copy below this cell. Then adapt the code to determine accuracy, precision, recall and F1-score for each classifier on each subset.\n",
    "\n",
    "Use the `sample` function from the random module, which means you should include the line:  \n",
    "`from random import sample`\n",
    "- Make sure that you are selecting samples that have an equal number of positive and negative reviews.\n",
    "\n",
    "Use a Pandas dataframe to display the results in a table.\n",
    "- The table should have nine columns:\n",
    " - C1 for the sample sizes, \n",
    " - C2-C5 for the Word List classifier performance metrics, and \n",
    " - C6-C9 for the Naïve Bayes classifier performance metrics.\n",
    "\n",
    "- You can use `pd.set_option('precision',2)` to limit the reals to have 2 digits after the decimal point.\n",
    "- Create a dataframe like this:\n",
    "```\n",
    "pd.DataFrame(list(zip(<column 1 list>, <column 2 list>, ...)),\n",
    "                  columns=<a list of the column headings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "\n",
    "Make a copy of the cell you created for the previous exercise and move it to be positioned below this cell. Using the new cell, repeat the above for each of the product categories.\n",
    "- The available categories are `'dvd'`, `'book'`, `'kitchen'` and `'electronics'`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "Interpret your results.  Specifically,\n",
    "1. What is the impact of the amount of training data on classifier performance?  \n",
    "2. Does this vary according to the classifier used?\n",
    "3. Does this vary according to the category of the data?\n",
    "4. Which classifier would you recommend to somebody else to use in their product? Are there any caveats or scenarios that you would warn them about (when it might not work as well as expected or a different classifier might be better?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
