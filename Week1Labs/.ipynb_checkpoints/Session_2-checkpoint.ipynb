{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure you have the following files in the default directory. This is easy to do using the notebook. The directory you were in when you opened the notebook is the default directory. By placing the files in the same directory you will not have to think about filepaths at all for these exercises.** \n",
    "\n",
    "* Exercise10.py\n",
    "\n",
    "* sample corpus.txt\n",
    "\n",
    "* sample corpus 2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We start with a simple exercise to see how to create a function and how to call it with an argument**\n",
    "\n",
    "* **The first block of text shows how doc strings are used by convention in Python. All functions should begin with a block of documentation (docstring) of the form given by the first block in triple quotes in the programme below.**\n",
    "* **Comments in the code itself are introduced by a # either as a separate line or appended to the end of a line. Python will ignore the rest of a line after a #.**\n",
    "\n",
    "**Type shift-enter to execute the cell. The function now exists in the kernel and can be called by any cell in the notebook. The function definition ends as soon as the indentation ceases (this is triggered by the comment \"Here is the argument:\").  After creating the function the kernel will continue to execute contents of the cell, thereby calling the function.** \n",
    "\n",
    "**Notice how the programme splits a character string at carriage return characters. This works because split is an inbuilt method of the text data type. Therefore all text objects can be split in this way.** \n",
    "* \"\\n\"** is the carriage return character.** \n",
    "* **Use **\"\\t\"** for reading tab separated files.**\n",
    "* **If you leave the argument empty it will treat any string of whitespace as a delimiter to be split. This has the advantage that a double space will be treated as a single delimiter.**\n",
    "* **Use the empty cell below to test what happens when you use **split(' ')** and encounter a double space.**\n",
    "\n",
    "**Next examine the contents of the variable** sample_text** with and without the print function.**\n",
    "* **Notice that execution of the first cell means the variable is now in the kernel and accessible to any cell.**\n",
    "* **Use the same box to try printing the variable** input_text**. What happens? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a sample sentence01 showing 7 different token types: alphabetic, numeric, alphanumeric, Title, UPPERCASE, CamelCase and punctuation!\\nSentences like that should not exist. They're too artificial.\\nA REAL sentence looks different. It has flavour to it. You can smell it; it's like Pythonic code, you know?\\nHave you heard of 'code smell'? Google it if you haven't.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def count_paragraphs(input_text):\n",
    "    \"\"\"\n",
    "    A paragraph is defined as the text before a CR character ie. \"\\n\"\n",
    "    Take a character string, split it into paragraphs, count them.\n",
    "    and return the counts\n",
    "    :param input_text: a character string containing paragraph marks\n",
    "    :return: integer, the number of paragraphs\n",
    "    \"\"\"\n",
    "    \n",
    "    # The following statement creates a list of strings by breaking\n",
    "    # up input_text wherever a \"\\n\" character occurs\n",
    "    \n",
    "    paragraphs = input_text.split(\"\\n\")  \n",
    "    \n",
    "    # The len() function counts the number of elements in the list\n",
    "    \n",
    "    return len(paragraphs)\n",
    "\n",
    "\n",
    "# Here is the argument:\n",
    "\n",
    "sample_text = \"This is a sample sentence01 showing 7 different token types: alphabetic, numeric, alphanumeric, Title, UPPERCASE, CamelCase and punctuation!\\nSentences like that should not exist. They're too artificial.\\nA REAL sentence looks different. It has flavour to it. You can smell it; it's like Pythonic code, you know?\\nHave you heard of 'code smell'? Google it if you haven't.\"\n",
    "print (sample_text)\n",
    "\n",
    "# Here is the function call:\n",
    "\n",
    "print (\"Number of paragraphs: \", count_paragraphs(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "**The next programme introduces the set datatype and shows how to create a vocabulary with it.**\n",
    "\n",
    "**A set is an unordered collection of unique objects. Like a list, a set is iterable. That means python's powerful capabilities for performing actions on successive elements can be used on it. Here we see one of the most basic and important of these, iterating with a for loop.**\n",
    "\n",
    "**Check you are comfortable with what the for loop is doing then:**\n",
    "* **change the function by adding a line so that the set returned by the function call is stored in a variable.**\n",
    "* **Spend a moment considering the shortcomings of this function.**\n",
    "* **Is the space character by itself a good word delimiter?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's\n",
      "that\n",
      "You\n",
      "not\n",
      "it;\n",
      "of\n",
      "if\n",
      "This\n",
      "numeric,\n",
      "It\n",
      "looks\n",
      "can\n",
      "Pythonic\n",
      "flavour\n",
      "types:\n",
      "is\n",
      "code,\n",
      "token\n",
      "smell\n",
      "it\n",
      "sentence01\n",
      "7\n",
      "smell'?\n",
      "CamelCase\n",
      "sentence\n",
      "alphanumeric,\n",
      "you\n",
      "different.\n",
      "Google\n",
      "showing\n",
      "like\n",
      "and\n",
      "alphabetic,\n",
      "different\n",
      "to\n",
      "UPPERCASE,\n",
      "A\n",
      "haven't.\n",
      "REAL\n",
      "know?\n",
      "a\n",
      "it.\n",
      "artificial.\n",
      "has\n",
      "sample\n",
      "They're\n",
      "exist.\n",
      "Have\n",
      "heard\n",
      "Sentences\n",
      "should\n",
      "'code\n",
      "too\n",
      "punctuation!\n",
      "Title,\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(input_text):\n",
    "    \"\"\"\n",
    "    A word is defined as a character string delimited by\n",
    "    a space \" \" character.\n",
    "    Given an input string, split it into words and\n",
    "    return the set of unique words in the input.\n",
    "    :param input_text: Character string with some text\n",
    "    :return: The set of unique words in the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_words = input_text.split()\n",
    "    \n",
    "    # The following line takes the list of words,\n",
    "    # removes repetitions and creates a set:\n",
    "\n",
    "    return set(list_of_words)\n",
    "\n",
    "# The following loop is repeated for every word in the set\n",
    "# Note that a set is just one of many iterable objects:\n",
    "\n",
    "\n",
    "for word in get_vocabulary(sample_text):\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another important data type is the dictionary, which maps keys onto values. Both keys and values can be a wide range of objects.  The programme below uses a special form of dictionary imported from a library with a few special features. First spend a moment experimenting with the native basic dictionary form in the empty cell below. One way to create a dictionary is:**\n",
    "\n",
    "my_dictionary = dict( ).\n",
    "\n",
    "**You can then assign values to keys like this (red is the key and colour is the value):**\n",
    "\n",
    "* my_dictionary['red'] = 'colour'.\n",
    "* my_dictionary['banana'] = 'noun'\n",
    "\n",
    "**Keys must be unique. There is no requirement for keys or values to be of the same type as other keys or values.\n",
    "Another way to creat a dictionary is**\n",
    "\n",
    "* my_dictionary = {'red': 'colour', 'banana': 'noun'}\n",
    "\n",
    "**Here we load the sample sample text from file. (It is the same text as before but loading from file is the usual way to access text, both as a matter of good practice and because text files can be very large).**\n",
    "* **Notice carefully how the programme opens a file. If the file isn't in the default directory you will need to set the file path.**\n",
    "* **Instead of using this form you can open a file with explicit open and close statements. Change the programme to do this. The format is **file_handle = open(   )** and then **file_handle.close( )**. The latter needs no argument.**\n",
    "\n",
    "* **The advantage of the first method is that the file will close automatically without you having to remember.**\n",
    "* **The indentation also shows you clearly when the file is in use. This could be an advantage or a disadvantage depending on how long the file is open and how many levels of indentation you are working with.**\n",
    "\n",
    "**The programme shows how to add and update entries in a dictionary and how to iterate over a dictionary extracting key-value pairs. Notice the import statement. What happens if you delete it?**\n",
    "\n",
    "**The special imported version of the dictionary has a default value. Once you are comfortable with what it does, convert the programme to use the basic dictionary type. You will now need to introduce code to check if a word is in the dictionary as you can no longer rely on the dictionary structure to automatically assign a 0 to unknown entries.**\n",
    "\n",
    "**To test whether an item is present use this:**\n",
    "\n",
    "* if 'red' in my_dictionary:\n",
    "      \n",
    "**Finally, look at the results obtained for the word \"it\" and think for a moment about the problem cause by the occurrence of \"it's\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 1\n",
      "is 1\n",
      "a 1\n",
      "sample 1\n",
      "sentence 2\n",
      "showing 1\n",
      "7 1\n",
      "different 1\n",
      "token 1\n",
      "types: 1\n",
      "alphabetic, 1\n",
      "numeric, 1\n",
      "alphanumeric, 1\n",
      "Title, 1\n",
      "UPPERCASE, 1\n",
      "CamelCase 1\n",
      "and 1\n",
      "punctuation! 1\n",
      "Sentences 1\n",
      "like 2\n",
      "that 1\n",
      "should 1\n",
      "not 1\n",
      "exist. 1\n",
      "They're 1\n",
      "too 1\n",
      "artificial 1\n",
      "A 1\n",
      "REAL 1\n",
      "looks 1\n",
      "different. 1\n",
      "It 1\n",
      "has 1\n",
      "flavour 1\n",
      "to 1\n",
      "it. 1\n",
      "You 1\n",
      "can 1\n",
      "smell 1\n",
      "it; 1\n",
      "it's 1\n",
      "Pythonic 1\n",
      "code, 1\n",
      "you 3\n",
      "know? 1\n",
      "Have 1\n",
      "heard 1\n",
      "of 1\n",
      "'code 1\n",
      "smell'? 1\n",
      "Google 1\n",
      "it 1\n",
      "if 1\n",
      "haven't. 1\n"
     ]
    }
   ],
   "source": [
    "# Identifiers that are not built-in and are defined somewhere\n",
    "# else need to be imported into this module, that is, included\n",
    "# as if they were part of this source module:\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "def count_words_from_file(input_text):\n",
    "    \"\"\"\n",
    "    A word is defined as a character string delimited by\n",
    "    a space \" \" character.\n",
    "    Given a file containing text, count how many unique\n",
    "    words it contains and return the counts.\n",
    "    :param file_path: A file path with the file name\n",
    "    :return: A dictionary containing entries {word:count}\n",
    "    \"\"\"\n",
    "   \n",
    "    # The dictionary with default values is defined in the\n",
    "    # collections module we imported. The int parameter\n",
    "    # tells Python that when a key is accessed that has not\n",
    "    # been previously stored in the dictionary, it should\n",
    "    # create an entry for this key with a value of 0:\n",
    "    \n",
    "    word_counts = collections.defaultdict(int)\n",
    "    for word in input_text.split():\n",
    "    \n",
    "        # increment the word's count, starting with 0 if it has no entry\n",
    "\n",
    "        word_counts[word] += 1  \n",
    "    return word_counts\n",
    "\n",
    "\n",
    "input_file_path = \"sample_corpus.txt\"\n",
    "with open(input_file_path) as input_file_handle:  # open the file for reading\n",
    "    sample_text = input_file_handle.read()  # read the entire file\n",
    "word_counts = count_words_from_file(sample_text)  \n",
    "\n",
    "# The items of a dictionary are key:value tuples, in this case they\n",
    "# are the word and its corresponding count; they can be accessed thusly:\n",
    "\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    print (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises 4 - 7 look at various different versions of a programme to count the sentences in our sample text.**\n",
    "\n",
    "**Notice how to create an empty list. You can also do it with **= [ ]** though it is now considered better practice to use **= list( ).\n",
    "\n",
    "**It will help your programming enormously if you can understand the difference between two commonly used inbuilt list methods, append and extend. append puts a single object at the end of a list. extend concatenates all the elements of the object. **\n",
    "\n",
    "**The argument of extend is often a list but it can be any iterable. Experiment with this in the empty cell below. Make sure you try appending and extending by both text and numbers, both single items and a list. The results are sometimes surprising. This is a common source of errors.**\n",
    "\n",
    "**A tuple is an immutable sequence of objects. It is like a list but you cannot change it. It can be of arbitrary length including 0 and 1. For example:**\n",
    "\n",
    "* a = tuple( )\n",
    "* a = (3,)\n",
    "* a = 3,\n",
    "\n",
    "**The last two are the same.**\n",
    "\n",
    "**When should you use a tuple and when a list? If the position of an item in a list matters it is probably better to use a tuple. If it doesn't matter, use a list.** \n",
    "\n",
    "**Now check what the programme below is doing and run it.Try to understand what is happening. ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence showing 7 different token types: alphabetic, numeric, alphanumeric, Title, UPPERCASE, CamelCase and punctuation!\n",
      "Sentences like that should not exist. They're too artificial\n",
      "A REAL sentence looks different. It has flavour to it. You can smell it; it's like Pythonic code, you know?\n",
      "Have you heard of 'code smell'? Google it if you haven't.\n",
      "[(1, 1), (2, 2), (3, 3), (4, 1)]\n"
     ]
    }
   ],
   "source": [
    "def count_sentences_per_paragraph(input_text):\n",
    "    \"\"\"\n",
    "    Given an input text:\n",
    "     - assign a number to each paragraph,\n",
    "     - count the number of sentences in each paragraph,\n",
    "     - output a list of all paragraph numbers together\n",
    "       with the number of sentences in it.\n",
    "\n",
    "    :param input_text: A character string possibly containing\n",
    "                        periods \".\" to separate sentences and\n",
    "                        paragraph marks \"\\n\" to separate\n",
    "                        paragraphs.\n",
    "    :return: A list of ordered pairs (tuples) where the first\n",
    "            element of the pair is the paragraph number and\n",
    "            the second element is the number of sentences in\n",
    "            that paragraph.\n",
    "            Sample output: [(0, 1), (1, 3), (2, 3), (3, 1)]\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = input_text.split(\"\\n\")\n",
    "    sentences_per_paragraph = list()  # create an empty list\n",
    "    paragraph_number = 0\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_number += 1\n",
    "        sentences = paragraph.split(\". \")\n",
    "        number_of_sentences = len(sentences)\n",
    "        \n",
    "        # create a tuple with the paragraph number and the number of sentences\n",
    "        # in it, then append the tuple to the list:\n",
    "        \n",
    "        sentences_per_paragraph.append((paragraph_number, number_of_sentences))\n",
    "    return sentences_per_paragraph\n",
    "\n",
    "\n",
    "print (sample_text)\n",
    "print (count_sentences_per_paragraph(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Study the code below. This is a reworking of the previous programme with the actual counting of sentences taken outside the programme into a separate function.**\n",
    "\n",
    "* **One of the reasons for doing this is to isolate a task which will be used in different places. That reason doesn't really apply here as the new function is only being called in one place.**\n",
    "* **Another reason for making a separate function is readability. Do you think this change improves readability?**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7e39120be6d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_sentences_per_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_text' is not defined"
     ]
    }
   ],
   "source": [
    "def count_sentences_per_paragraph(input_text):\n",
    "    \"\"\"\n",
    "    Given an input text:\n",
    "     - assign a number to each paragraph,\n",
    "     - count the number of sentences in each paragraph,\n",
    "     - output a list of all paragraph numbers together\n",
    "       with the number of sentences in it.\n",
    "\n",
    "    :param input_text: A character string possibly containing\n",
    "                        periods \".\" to separate sentences and\n",
    "                        paragraph marks \"\\n\" to separate\n",
    "                        paragraphs.\n",
    "    :return: A list of ordered pairs (tuples) where the first\n",
    "            element of the pair is the paragraph number and\n",
    "            the second element is the number of sentences in\n",
    "            that paragraph.\n",
    "            Sample output: [(0, 1), (1, 3), (2, 3), (3, 1)]\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = input_text.split(\"\\n\")\n",
    "    sentences_per_paragraph = []\n",
    "    paragraph_number = 0\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_number += 1\n",
    "        number_of_sentences = count_sentences(paragraph)\n",
    "        sentences_per_paragraph.append((paragraph_number, number_of_sentences))\n",
    "    return sentences_per_paragraph\n",
    "      \n",
    "    #sentences_per_paragraph = \n",
    "    #enumerated_\n",
    "    #return enumerated_sentences_per_paragraph\n",
    "\n",
    "def count_sentences(paragraph):\n",
    "    \"\"\"\n",
    "    A sentence is a character string delimited by a period \".\"\n",
    "    Given an input paragraph, return the number of sentences\n",
    "    in it.\n",
    "    :param paragraph: Character string with sentences.\n",
    "    :return: number of sentences in the input paragraph\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = paragraph.split(\".\")\n",
    "    return len(sentences)\n",
    "\n",
    "\n",
    "print(count_sentences_per_paragraph(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "\n",
    "**First experiment with **zip( )**, a function which often proves useful and is worth remembering. Try copying these lines in the empty cell:**\n",
    "* print zip(['a','b','c'], range(5))\n",
    "* for a,b in zip(['a','b','c'], range(5)): print a,b\n",
    "\n",
    "**Study the next version of the same programme.**\n",
    "* **We now make a list of consecutive integers using** range( ).\n",
    "    \n",
    "* **We then combine two lists into one .**\n",
    "* **In this case the use of **range( )** in combination with **zip( )** is quite neat. However, it does create an unnecessary list of integers and there are situations where this approach gets messy.**\n",
    "\n",
    "**Instead of counting the paragraphs we can get python to do it for us using an inbuilt function with precisely that purpose:**\n",
    "* enumerate( )** performs the combination of **range( )** and **zip( )** automatically.**\n",
    "* **Experiment with **enumerate( )** in an empty cell. For example you could try:**\n",
    "* for a,b in enumerate(['The','Holy','Grail']): print a,b\n",
    "\n",
    "**Change the programme below to use **enumerate( )** instead of **range( )** and **zip( ). **The code you need to insert is:**\n",
    "\n",
    "* sentences_per_paragraph = [(ind,val) for ind,val in enumerate(sentence_counts, 1)]\n",
    "\n",
    "**There are two ways this can be done. You can either insert the enumerate command in the function (which will require fewer changes) or you can make the function return a simple list instead of a list of tuples and use enumerate in the print command instead of in the function itself. This would dedicate the function to its real task of getting the list, leaving the presentation as a separate task.**\n",
    "\n",
    "**Try running it without the second argument (i.e. just use** enumerate(sentence_counts)** without the 1. Check you understand the difference.**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sentences_per_paragraph(input_text):\n",
    "    \"\"\"\n",
    "    Given an input text:\n",
    "     - assign a number to each paragraph,\n",
    "     - count the number of sentences in each paragraph,\n",
    "     - output a list of all paragraph numbers together\n",
    "       with the number of sentences in it.\n",
    "\n",
    "    :param input_text: A character string possibly containing\n",
    "                        periods \".\" to separate sentences and\n",
    "                        paragraph marks \"\\n\" to separate\n",
    "                        paragraphs.\n",
    "    :return: A list of ordered pairs (tuples) where the first\n",
    "            element of the pair is the paragraph number and\n",
    "            the second element is the number of sentences in\n",
    "            that paragraph.\n",
    "            Sample output: [(0, 1), (1, 3), (2, 3), (3, 1)]\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = input_text.split(\"\\n\")\n",
    "    sentence_counts = []\n",
    "    for paragraph in paragraphs:\n",
    "        number_of_sentences = count_sentences(paragraph)\n",
    "        sentence_counts.append(number_of_sentences)\n",
    "    \n",
    "    # Create a list with the paragraph numbers we need:\n",
    "    \n",
    "    paragraph_numbers = range(len(paragraphs))\n",
    "    \n",
    "    # Make a list of tuples by combining two existing lists:\n",
    "    sentences_per_paragraph = zip(paragraph_numbers, sentence_counts)\n",
    "    return sentences_per_paragraph\n",
    "\n",
    "\n",
    "print count_sentences_per_paragraph(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The final version of the programme demonstrates the use of the** map( ) **function to iterate over a list. The advantage of this is that it is no longer necessary to initialize the list and append elements to it in a loop. **\n",
    "\n",
    "**When used simply like here, the map function is good python but it can be used to write complicated code which is difficult to read and is considered poor style. Where possible it is usually good practice to use list comprehensions.**\n",
    "\n",
    "**Change the programme below to use a list comprehension instead of** map( )\n",
    "\n",
    "**The code you need is:**\n",
    "* sentence_counts = [count_sentences(paragraph) for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sentences_per_paragraph(input_text):\n",
    "    \"\"\"\n",
    "    Given an input text:\n",
    "     - assign a number to each paragraph,\n",
    "     - count the number of sentences in each paragraph,\n",
    "     - output a list of all paragraph numbers together\n",
    "       with the number of sentences in it.\n",
    "\n",
    "    :param input_text: A character string possibly containing\n",
    "                        periods \".\" to separate sentences and\n",
    "                        paragraph marks \"\\n\" to separate\n",
    "                        paragraphs.\n",
    "    :return: A list of ordered pairs (tuples) where the first\n",
    "            element of the pair is the paragraph number and\n",
    "            the second element is the number of sentences in\n",
    "            that paragraph.\n",
    "            Sample output: [(0, 1), (1, 3), (2, 3), (3, 1)]\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = input_text.split(\"\\n\")\n",
    "    \n",
    "    # Apply the count_sentences function to every element of paragraphs,\n",
    "    # return the results in a new list, call it sentence_counts:\n",
    "    \n",
    "    sentence_counts = map(count_sentences, paragraphs)\n",
    "    paragraph_numbers = range(len(paragraphs))\n",
    "    sentences_per_paragraph = zip(paragraph_numbers, sentence_counts)\n",
    "    return sentences_per_paragraph\n",
    "\n",
    "\n",
    "print count_sentences_per_paragraph(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have now done the following:**\n",
    "\n",
    "* **Written a programme and checked it worked**\n",
    "\n",
    "* **Written several more versions of it and checked they work too**\n",
    "\n",
    "**Now it is time to think about checking and testing. There is in fact a problem with the programme. In a real programming situation of course you do not know whether there is a problem but try to imagine you haven't been told. How hard would you have checked? The purpose of this exercise is to consider different ways of checking for problems and to show how difficult it can be. The same problem exists in all our versions of this programme**\n",
    "\n",
    "* **First look at the code and see if you can see where the problem lies. It is often impossible to find problems by inspection.**\n",
    "\n",
    "* **Next, do some experimenting with the split function. Do you really understand how it works?**\n",
    "\n",
    "* **The best way to find problems is usually to test the programme thoroughly. See if you can find the problem by testing it more carefully.**\n",
    "\n",
    "* **Load the file **sample_corpus_2.txt** and run the programme on it. (It is best to carry out a separate experiment in a new cell but if you find this difficult, you can change the file name in exercise 3, execute the cell again, then run the programme with the new file stored in the old variable name, taking care to change it back and execute it again afterwards).**\n",
    "\n",
    "* **Study the input and output until you understand what the problem was. Notice the number of outputs generated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next few exercises develop a programme for making tokens. A token is a specific occurrence of a basic unit of lexical processing, typically a word or an item of punctuation.**\n",
    "\n",
    "* **Study the programme, in particular the string methods. These are very useful in NLP.**\n",
    "* **Experiment with the string methods using the empty cell until you understand how they work in special cases such as a single space and a single punctuation mark.**\n",
    "* **The programme will only assign one feature to each token. Are there any cases where more than one feature should be assigned?**\n",
    "\n",
    "* **How does the programme delimit tokens (i.e. find where one ends and the next begins)? Look at the results and note how poorly this works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tokens(input_text):\n",
    "    \"\"\"\n",
    "    Take an input text, split it into tokens, find the\n",
    "    token's shape, make a feature\n",
    "    vector with the token itself and its shape, return\n",
    "    a list of all token feature vectors found in the input.\n",
    "    :param input_text: A character string containing spaces\n",
    "    :return: A list of token feature vectors (token, shape).\n",
    "        Sample output: [('a', 'alpha'), ('7', 'digit'), ('A27', 'alnum')]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we define a token as being delimited by a space character:\n",
    "    \n",
    "    tokens = input_text.split(\" \")\n",
    "    return map(make_token_feature_vector, tokens)\n",
    "\n",
    "\n",
    "def make_token_feature_vector(token):\n",
    "    \"\"\"\n",
    "    Given a token, extract its shape and return a\n",
    "    vector with the token itself and its shape\n",
    "    :param token: A character string\n",
    "    :return: A tuple (token, shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    if token.isalpha():\n",
    "        return (token, \"alpha\")\n",
    "    elif token.isdigit():\n",
    "        return (token, \"digit\")\n",
    "    elif token.isalnum():\n",
    "        return (token, \"alnum\")\n",
    "    elif token in \",:;\":  \n",
    "        return (token, \"punctuation\")\n",
    "    elif token in \".!?\":  \n",
    "        return (token, \"sentence_end\")\n",
    "    elif token == \"\\n\":  \n",
    "        return (token, \"paragraph_end\")\n",
    "    else:\n",
    "        return (token, \"other\")\n",
    "\n",
    "\n",
    "for token in make_tokens(sample_text):\n",
    "    print token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelim to exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9 introduces lazy generators, an important form of function in python. A lazy generator does not calculate its results all at once but returns them one a a time for iteration. THe **enumerate** function which we saw in example 6 is a lazy generator.**\n",
    "\n",
    "**You can define lazy generator functions by using **yield** instead of **return**. When the function reaches a **yield** command it yields the argument and suspends execution without terminating and returns control to the level that called the function. The next time it is called it it resumes from the same place that it was left. There is no requirement to have a single yield command. You can yield in one place the first time and another place the next time (as you will see from the programme in the exercise).**\n",
    "\n",
    "**The cell below shows a simple function using both forms so that you can see the difference. Notice that you cannot use the result in the same way. A result that is returned is passed directly as value whereas a result that is yielded must be used in an iterator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_count_to_ten():\n",
    "    return range(1,11)\n",
    "\n",
    "\n",
    "def yield_count_to_ten():\n",
    "    for i in range(1, 11):\n",
    "        yield i\n",
    "\n",
    "        \n",
    "l = return_count_to_ten()\n",
    "print l\n",
    "    \n",
    "i = yield_count_to_ten()\n",
    "print ('yield')\n",
    "print i\n",
    "\n",
    "l = list(yield_count_to_ten())\n",
    "print l\n",
    "\n",
    "for i in yield_count_to_ten():\n",
    "    print i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The previous programme delimited tokens by looking for spaces between them. You should have noticed that it doesn't work very well because it doesn't account for punctuation symbols. We need a better way to do this and, ideally, a separate function to do it.** \n",
    "\n",
    "**Because it is hard to follow, here is a summary of the logic of the new function,** split_tokens(input_text):\n",
    "\n",
    "**The function reads the whole string one character at a time, adding characters to the token variable.**\n",
    "* **When it encounters a delimiter it yields the token.**\n",
    "* **If the token is empty it yields the delimiter character - unless it is a space - because the delimiter is an item of punctuation which is itself a token.**\n",
    "* **After returning a token the variable is reset to an empty string.**\n",
    "\n",
    "**Notice how the function yields the result instead of returning it. This means that it continues from the same point next time it is called.**\n",
    "* **Try calling the function using the empty cell. What happens?**\n",
    "* **Notice that the programme does not make a simple function call, it uses it in a list comprehension which iterates over it. Another common way to collect the yields would be with a for loop.**\n",
    "\n",
    "**To make sure you understand the logic tests, experiment with statements of the form:**\n",
    "* if variable_name: print \"True\"\n",
    "\n",
    "**Test this with the variable set to different types of data, including an empty list and an empty string.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_tokens(input_text):\n",
    "    \"\"\"\n",
    "    Take an input text, split it into tokens, find the\n",
    "    token's shape, make a feature\n",
    "    vector with the token itself and its shape, return\n",
    "    a list of all token feature vectors found in the input.\n",
    "    :param input_text: A character string containing spaces\n",
    "    :return: A list of token feature vectors (token, shape).\n",
    "        Sample output: [('a', 'alpha'), ('7', 'digit'), ('A27', 'alnum')]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Now it's up to the split_tokes function to decide what a token is.\n",
    "    # List comprehension creates a list by extracting elements from\n",
    "    # an iterable object, in this case Python automatically converts the\n",
    "    # split_tokens function into an iterable object because it uses the \"yield\" statement:\n",
    "    \n",
    "    tokens = [token for token in split_tokens(input_text)]\n",
    "    return map(make_token_feature_vector, tokens)\n",
    "\n",
    "\n",
    "def split_tokens(input_text):\n",
    "    \"\"\"\n",
    "    This function decides how to delimit a token. It takes an input\n",
    "    string, iterates over it character by character; it collects\n",
    "    constituent characters in the output token; punctuation characters\n",
    "    are considered delimiters therefore become tokens of their own; the\n",
    "    space character is removed from tokens. Yield each found token at\n",
    "    a time.\n",
    "    :param input_text: A character string containing a mix of text and delimiter characters.\n",
    "    :yield A character string which is either free from delimiters or\n",
    "        is a delimiter itself.\n",
    "    \"\"\"\n",
    "\n",
    "    DELIMITERS = \",:!?.\\n\"\n",
    "    token = \"\"\n",
    "    for char in input_text:\n",
    "        if char in DELIMITERS:  # test if the input character is a delimiter (substring presence)\n",
    "            \n",
    "            # Character strings, lists, etc, have a logical truth value in Python;\n",
    "            # an empty string is False, if it has characters it is True.\n",
    "            \n",
    "            if not token:  # same as token == \"\"\n",
    "                yield char\n",
    "            else:\n",
    "                \n",
    "                # Return token to the calling program, but next time this function\n",
    "                # is called, continue from\n",
    "                # the next statement rather than from the beginning of the function:\n",
    "                \n",
    "                yield token  # After yielding control to the calling program,\n",
    "                             # this function will execute the next statement:\n",
    "                token = \"\"  # Pick up execution from here.\n",
    "                yield char\n",
    "        elif char == \" \":\n",
    "            if token:  # same as token != \"\"\n",
    "                yield token\n",
    "                token = \"\"\n",
    "        else:\n",
    "            token += char\n",
    "\n",
    "for token in make_tokens(sample_text):\n",
    "    print token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this exercise is to learn the difference between three different ways of running a python programme.** \n",
    "\n",
    "**The first is the way used in these exercises: simply typing or pasting the code into a notebook (or console) and running it.** \n",
    "\n",
    "**Very similar to the first way is to import the code from a file or module into a notebook (or console). If you import a module, python will automatically run it. That means it reads every line in the file and executes. If the module contains function definitions, executing them means creating the functions. If it contains code that calls functions, python will make those calls and run the functions.**  \n",
    "\n",
    "**The third way is to run the module from the command line by typing python (or ipython) followed by the module name including the .py suffix.**\n",
    "\n",
    "**Python behaves the same for the second and third method. However, it is often useful to have a module that runs using the third method but doesn't run using the second i.e. you can import the functions, and perhaps some variables, without running anything. To achieve this, modules often include the line**\n",
    "* if __name__ == \"__main__\" **as in the cell below.**\n",
    "\n",
    "**This will run when called from the command line but not when the file is imported.**\n",
    "\n",
    "**The cell below contains the programmes for the tokens exercise we just looked at. It is also stored in a file named \"Exercise10.py\" You don't need to read the code as nothing has changed (apart from the addition of one line for testing which was added only to the saved file). Try all three methods:**\n",
    "\n",
    "**1. Execute the cell below**\n",
    "\n",
    "**2. In the empty cell execute:**\n",
    "\n",
    "* Import Exercise10 \n",
    "\n",
    "* **(Note the capital letter). It should not run the programme. To see what has happened, run the following commands: **\n",
    "\n",
    "* print noone\n",
    "\n",
    "* print Exercise10.noone\n",
    "\n",
    "* from Exercise10 import noone\n",
    "\n",
    "* print noone\n",
    "\n",
    "** The variable **noone** did not exist in the original programme (it was assigned in the test line that was added to the file).**\n",
    "* **Notice the difference between the two types of import. Using the second type is more convenient as you don't have to specify the namespace to access functions and variables.**\n",
    "* **For this reason people sometimes use the command** from module import \\*. **However, this is dangerous as you can easily overwrite existing names and python will not warn you. Using the import command in this way is considered bad practice. You can sometimes get away with it when importing your own module but avoid it with library modules.**\n",
    "\n",
    "**3. Run it from the command line** (ipython Exercise10.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_tokens(input_text):\n",
    "    \"\"\"\n",
    "    Take an input text, split it into tokens, find the\n",
    "    token's shape, make a feature\n",
    "    vector with the token itself and its shape, return\n",
    "    a list of all token feature vectors found in the input.\n",
    "    :param input_text: A character string containing spaces\n",
    "    :return: A list of token feature vectors (token, shape).\n",
    "        Sample output: [('a', 'alpha'), ('7', 'digit'), ('A27', 'alnum')]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Now it's up to the split_tokes function to decide what a token is.\n",
    "    # List comprehension creates a list by extracting elements from\n",
    "    # an iterable object, in this case Python automatically converts the\n",
    "    # split_tokens function into an iterable object because it uses the \"yield\" statement:\n",
    "    \n",
    "    tokens = [token for token in split_tokens(input_text)]\n",
    "    return map(make_token_feature_vector, tokens)\n",
    "\n",
    "\n",
    "def make_token_feature_vector(token):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a token, extract its shape and return a\n",
    "    vector with the token itself and its shape\n",
    "    :param token: A character string\n",
    "    :return: A tuple (token, shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    if token.isalpha():\n",
    "        return (token, \"alpha\")\n",
    "    elif token.isdigit():\n",
    "        return (token, \"digit\")\n",
    "    elif token.isalnum():\n",
    "        return (token, \"alnum\")\n",
    "    elif token in \",:;\":  \n",
    "        return (token, \"punctuation\")\n",
    "    elif token in \".!?\":  \n",
    "        return (token, \"sentence_end\")\n",
    "    elif token == \"\\n\":  \n",
    "        return (token, \"paragraph_end\")\n",
    "    else:\n",
    "        return (token, \"other\")\n",
    "\n",
    "\n",
    "\n",
    "def split_tokens(input_text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function decides how to delimit a token. It takes an input\n",
    "    string, iterates over it character by character; it collects\n",
    "    constituent characters in the output token; punctuation characters\n",
    "    are considered delimiters therefore become tokens of their own; the\n",
    "    space character is removed from tokens. Yield each found token at\n",
    "    a time.\n",
    "    :param input_text: A character string containing a mix of text and delimiter characters.\n",
    "    :yield A character string which is either free from delimiters or\n",
    "        is a delimiter itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    # First decide what characters delimit a token:\n",
    "    DELIMITERS = \",:!?.\\n\"\n",
    "    \n",
    "    token = \"\"\n",
    "    for char in input_text:\n",
    "        \n",
    "        if char in DELIMITERS:  # test if the input character is a delimiter (substring presence)\n",
    "            \n",
    "            # Character strings, lists, etc, have a logical truth value in Python;\n",
    "            # an empty string is False, if it has characters it is True.\n",
    "            \n",
    "            if not token:  # same as token == \"\"\n",
    "                yield char\n",
    "            else:\n",
    "                \n",
    "                # Return token to the calling program, but next time this function\n",
    "                # is called, continue from\n",
    "                # the next statement rather than from the beginning of the function:\n",
    "                \n",
    "                yield token  # After yielding control to the calling program,\n",
    "                             # this function will execute the next statement:\n",
    "                token = \"\"  # Pick up execution from here.\n",
    "                yield char\n",
    "        elif char == \" \":\n",
    "            if token:  # same as token != \"\"\n",
    "                yield token\n",
    "                token = \"\"\n",
    "        else:\n",
    "            token += char\n",
    "            \n",
    "sample_text = \"This is a sample sentence01 showing 7 different token types: alphabetic, numeric, alphanumeric, Title, UPPERCASE, CamelCase and punctuation!\\nSentences like that should not exist. They're too artificial.\\nA REAL sentence looks different. It has flavour to it. You can smell it; it's like Pythonic code, you know?\\nHave you heard of 'code smell'? Google it if you haven't.\"            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for token in make_tokens(sample_text):\n",
    "        print token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on terminology. The word \"parse\" means to read and process sequentially. In NLP it also has a specific meaning to analyse text to determine its syntax. To avoid confusion please be aware that in this exercise the first meaning is used.**\n",
    "\n",
    "**The programme below constructs a nested list by reading some input text and looking for delimiters.**\n",
    "* **First it runs our **make_tokens** function.**\n",
    "* **Then it reads one token at a time to construct sentences. Each sentence is a list.**\n",
    "* **When the end of a sentence is reached, a new empty list is created and a new sentence read.**\n",
    "* **When it reaches the end of a paragraph, all the sentences in that paragraph are kept together in a list and a new pargraph is created.**\n",
    "\n",
    "**There are various ways this could be done. The method below uses the generator method we have seen before where results are delivered using the yield command, instead of the return command. This means the function does not exit but resumes from the same place the next time it is called.**\n",
    "* **Using generators is often a good way to write clear simple code**\n",
    "* **Another advantages of the generator method is that it enables data to be processed as it is needed, making it possible to process very large lists that might use up too much memory.**\n",
    "\n",
    "**First, execute the cell and study the code until you understand how it works.** \n",
    "\n",
    "**Second consider how else this task could have been written. What do you think of this method? Is it easy to read?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_text(input_text):\n",
    "    \"\"\"\n",
    "    A parsed text is defined as a list of parsed paragraphs.\n",
    "    Given an input text, parse its paragraphs and return a list\n",
    "    with the results.\n",
    "    :param input_text: A character string with paragraphs\n",
    "    :return: A list of parsed paragraphs\n",
    "    \"\"\"\n",
    "    \n",
    "    return [paragraph for paragraph in parse_paragraphs(input_text)]\n",
    "\n",
    "\n",
    "def parse_paragraphs(input_text):\n",
    "    \"\"\"\n",
    "    A parsed paragraph is defined as a list of parsed sentences.\n",
    "    Given an input text, parse its sentences; if the sentence is\n",
    "    actually the end of a paragraph, then yield the previous\n",
    "    sentences packed as a list.\n",
    "    :param input_text: a character string containing paragraphs\n",
    "                       and sentences.\n",
    "    :yield: A list of sentences up to the end of the paragraph.\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraph = list()\n",
    "    for sentence in parse_sentences(input_text):\n",
    "        \n",
    "        # We expect parse_sentences to return \"paragraph_end\"\n",
    "        # when it encounters an end of paragraph mark.\n",
    "        \n",
    "        if sentence == \"paragraph_end\":\n",
    "            yield paragraph\n",
    "            paragraph = list()\n",
    "        else:\n",
    "            paragraph.append(sentence)\n",
    "    yield paragraph\n",
    "\n",
    "\n",
    "def parse_sentences(input_text):\n",
    "    \"\"\"\n",
    "    A parsed sentence is defined as a list of token vectors\n",
    "    :param input_text: a character string containing paragraphs,\n",
    "                       sentences and token vectors.\n",
    "    :yield: A list of token vectors up to the end of a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    token_vectors = make_tokens(input_text)  \n",
    "    sentence = list()\n",
    "    \n",
    "    # Since a token vector is a tuple (token, shape) we can unpack it\n",
    "    # automatically as we iterate over the list of token vectors:\n",
    "    \n",
    "    for token, shape in token_vectors:\n",
    "        if shape == \"sentence_end\":\n",
    "            yield sentence\n",
    "            sentence = list()\n",
    "        elif shape == \"paragraph_end\":\n",
    "            if sentence:\n",
    "                yield sentence\n",
    "                sentence = list()\n",
    "            yield \"paragraph_end\"\n",
    "        else:\n",
    "            sentence.append((token, shape))\n",
    "    if sentence:\n",
    "        yield sentence\n",
    "\n",
    "\n",
    "\n",
    "print \"************************** SENTENCES IN THE PARSED TEXT:\"\n",
    "for sentence in parse_sentences(sample_text):\n",
    "    print sentence\n",
    "print \"************************** PARAGRAPHS IN THE PARSED TEXT:\"\n",
    "\n",
    "for paragraph in parse_paragraphs(sample_text):\n",
    "    print paragraph\n",
    "\n",
    "print \"************************** PARSED TEXT:\"\n",
    "print parse_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The programme in this exercise selects a character at random from the nested list generated by the previous programme. Run the programme and make sure you can understand what it is doing.**\n",
    "\n",
    "**1. Recall that we defined a token vector to be an ordered pair (token, shape). Accessing the token or the shape with the code** token_vector[0]** or **token_vector[1]** is difficult to read. It is better to define the indices as constants. Constants are always given capitalised names and sit in the global scope. Do you agree that this improves readability?**\n",
    "\n",
    "**2. Notice how to index into the nested list and the character string. The line indexing the character string could have been written as:**\n",
    "\n",
    "character = parsed_text[paragraph_coord][sentence_coord][token_coord][TOKEN][character_coord]\n",
    "\n",
    "**Do you think this would have made the programme more readable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "TOKEN = 0  \n",
    "SHAPE = 1\n",
    "\n",
    "def get_random_character_coordinates_in_text(parsed_text):\n",
    "    \"\"\"\n",
    "    Given a parsed text, as the one produced by parse_text.py,\n",
    "    return a random character within the text, together with its\n",
    "    coordinates.\n",
    "    :param parsed_text: A nested list with token vectors within\n",
    "        sentence lists within paragraph lists.\n",
    "    :return: A vector where the elements are: the random character,\n",
    "        the paragraph, sentence, token and character coordinates.\n",
    "        Sample output: ('f', 3, 1, 2, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a random index within a valid range:\n",
    "    \n",
    "    paragraph_coord = random.randrange(len(parsed_text))\n",
    "    sentence_coord = random.randrange(len(parsed_text[paragraph_coord]))\n",
    "    token_coord = random.randrange(len(parsed_text[paragraph_coord][sentence_coord]))\n",
    "    token = parsed_text[paragraph_coord][sentence_coord][token_coord][TOKEN]\n",
    "    character_coord = random.randrange(len(token))\n",
    "        \n",
    "    \n",
    "    # With the obtained random coordinates, access the input parsed text:\n",
    "    \n",
    "    character = token[character_coord]\n",
    "    \n",
    "    return character, paragraph_coord, sentence_coord, token_coord, character_coord\n",
    "\n",
    "\n",
    "parsed_text = parse_text(sample_text)\n",
    "for _ in range(10):\n",
    "    print get_random_character_coordinates_in_text(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
