{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 (Part 2): Dictionary Methods for WSD\n",
    "\n",
    "We have seen that many words have many different senses.  In order to make the correct decision about the meaning of a sentence or a document, an application often needs to be able to **disambiguate** individual words, that is, choose the correct sense given the context.\n",
    "\n",
    "In this lab we will be looking st methods for word sense disambiguation (WSD) that make use of dictionaries or other lexical resources (also referred to as **knowledge-based methods** for WSD).  In particular, we will look at\n",
    "* simplified Lesk\n",
    "* adapted Lesk\n",
    "* minimising distance in a semantic hierarchy\n",
    "\n",
    "As in the previous lab, we will be using WordNet as our lexical resource.  So, first, lets import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wn_ic\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import operator, sys\n",
    "\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/juliewe/Documents/teaching/NLE2018/resources')\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that the path to your utils.py file is correct for your computer\n",
    "sys.path.append('/Users/juliewe/Documents/teaching/NLE/NLE2019/w4/Week4Labs')\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Lesk\n",
    "\n",
    "The Lesk algorithm is based on the intuition that the correct combination of senses in a sentence will share more common words in their definitions.\n",
    "\n",
    "It is computationally very expensive to compare all possible sense combinations of words in a sentence.  If each word has just 2 senses, then there are $2^n$ possible sense combinations.\n",
    "\n",
    "In the simplifed Lesk algorithm, below, we consider each word in turn and choose the sense whose definition has more **overlap** with the contextual words in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplifiedLesk(word,sentence):\n",
    "    '''\n",
    "    Use the simplified Lesk algorithm to disambiguate word in the context of sentence\n",
    "    word: a String which is the word to be disambiguated\n",
    "    sentence: a String which is the sentence containing the word\n",
    "    :return: a pair (chosen sense definition, overlap score)\n",
    "    '''\n",
    "    \n",
    "    #construct the set of context word tokens for the sentence: all words in sentence - word itself\n",
    "    contexttokens=set(word_tokenize(sentence))-{word}\n",
    "    \n",
    "    #get all the possible synsets for the word\n",
    "    synsets=wn.synsets(word)\n",
    "    scores=[]\n",
    "    \n",
    "    #iterate over synsets\n",
    "    for synset in synsets:\n",
    "        #get the set of tokens in the definition of the synset\n",
    "        sensetokens=set(word_tokenize(synset.definition()))\n",
    "        #find the size of the intersection of the sensetokens set with the contexttokens set\n",
    "        scores.append((synset.definition(),len(sensetokens.intersection(contexttokens))))\n",
    "    \n",
    "    #sort the score list in descending order by the score (which is item with index 1 in the pair)\n",
    "    sortedscores=sorted(scores,key=operator.itemgetter(1),reverse=True) \n",
    "    #print(sortedscores)\n",
    "    return sortedscores[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test it on a couple of sentences containing the word *bank*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksentences=[\"he borrowed money from the bank\",\"he sat on the bank of the river and watched the currents\"]\n",
    "for sentence in banksentences:\n",
    "    print(sentence,\":\",simplifiedLesk(\"bank\",sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually appears not to do too bad.  However, this is more by luck than anything else.   If you inspect the sentences and the definitions, you will notice that most of the overlap is currently generated by stopwords.\n",
    "\n",
    "### Exercise 1.1\n",
    "Improve the SimplifiedLesk algorithm by carrying out:\n",
    "* case and number normalisation \n",
    "* stopword filtering\n",
    "* lemmatisation\n",
    "\n",
    "You should find some useful functions for doing this in `utils.py` based on earlier labs.\n",
    "\n",
    "Make sure you test it.  Unfortunately, you should now find 0 overlap between any of the senses and the two bank sentences given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted Lesk\n",
    "WordNet definitions are very short.  However, it is possible to create a bigger set of sense words by including information about the hypernyms and hyponyms of each sense.\n",
    "\n",
    "### Exercise 2.1\n",
    "Adapt the Lesk algorithm to include in `sensetokens`:\n",
    "* all of the lemma_names for the sense itself\n",
    "* all of the lemma_names for the hypernyms of the sense\n",
    "* all of the lemma_names for the hypoynyms of the sense\n",
    "* all of the words from the definitions of the hypernyms of the sense\n",
    "* all of the words from the definitions of the hyponyms of the sense\n",
    "\n",
    "Make sure you carry out normalisation and lemmatisation of these words as before\n",
    "\n",
    "Test each adaptation you make on the bank sentences, recording the overlap observed with the chosen sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "* From a sample of 1000 sentences from the dvd category of the Amazon review corpus (using the `sample_raw_sents()` method), find sentences which contain the lemma *film*. It will depend on the exact sample, but I would expect there to be somewhere between 50 and 100. \n",
    "* Use your AdaptedLesk algoritm to disambiguate them.  You may want to adapt it slightly so that it takes as input a list or a set of context lemmas rather than the sentence itself.  \n",
    "* Record the number of instances of each sense of *film* predicted by this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvd_reader = AmazonReviewCorpusReader().category(\"dvd\")\n",
    "sentences=dvd_reader.sample_raw_sents(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "Inspect some of the individual predictions for your film sentences (at least one for each sense predicted).  Do you agree with the sense prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the Distance in the Semantic Hierarchy\n",
    "This WSD method is based on the intuition that the concepts mentioned in a sentence will be close together in the hyponym hierarchy.\n",
    "\n",
    "### Exercise 3.1\n",
    "Write a function `max_sim(word, contextlemmas,pos)`which will choose the sense of a *word* given its context *sentence* using a WordNet based semantic similarity measure (see Lab_5_1).  You can assume that the part of speech of the word is known and is supplied to the function as another argument.\n",
    "\n",
    "Within the function, \n",
    "1. For each **sense** of the word under consideration:\n",
    "* compute its semantic similarity with each context **lemma** of the same part of speech.  For each context lemma you will need to consider each of its **senses** (and take the maximum similarity).  Therefore, you will need a triple nested loop! \n",
    "* sum the semantic similarities over the sentence\n",
    "2. Choose the **sense** with the maximum sum.\n",
    "\n",
    "Test your function on the bank sentences.  You should find, disappointingly for the method,  that the first sentence has a maximum score of 2.71 with \"an arrangement of similar objects in a row or in tiers\" and the second sentence has a maximum socre of 4.68 with \"an arrangement of similar objects in a row or in tiers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "* Run your max_sim function on all of your film sentences and record the number of predictions for each sense.\n",
    "* Inspect some of the individual predictions.\n",
    "* Compare the results with those from the AdaptedLesk algorithm and draw some conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
