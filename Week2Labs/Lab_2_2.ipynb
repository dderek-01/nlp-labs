{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Preprocessing Text (Part 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex NLTK root directory is \\Users\\J\\Desktop\\code\\sussex\\nlp\\labs\\resources\n"
     ]
    }
   ],
   "source": [
    "#necessary library imports and setup introduced previously\n",
    "\n",
    "import sys\n",
    "#sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering') \n",
    "#sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'\\Users\\J\\Desktop\\code\\sussex\\nlp\\labs\\resources')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sussex_nltk.corpus_readers import ReutersCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "Remember, a raw text document is just a sequence of characters. There are a number of basic steps that are often performed when processing natural language text. In lab sessions this week we are covering some of the basic text pre-processing methods. Last time, you looked at\n",
    "- <b> segmentation</b> - breaking down large units of text into smaller units such as documents and sentences. \n",
    "- <b> tokenisation</b> - roughly speaking, this involves grouping characters into words;\n",
    "\n",
    "This time, you will be looking at:\n",
    "- <b>case normalisation</b> - this involves converting all of the text into lower case; \n",
    "- <b>stemming</b> - this involves removing a word's inflections to find the stem; and \n",
    "- <b>punctuation and stop-word removal</b> - stop-words are common functions words that in some situations can be ignored.\n",
    "\n",
    "Note that we do not always apply all of the above preprocessing methods; it depends on the application. One of the things that you will be learning about in this module, is when the application of each of these methods is, and is not, appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising text and removing unimportant tokens\n",
    "In this next section we will consider several methods that pre-process (tokenised) text in ways that are sometimes helpful to 'downstream' processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number and case normalisation\n",
    "Without any kind of normalisation, the tokens `\"help\"` and `\"Help\"` are two distinct types. In some contexts you may not want to distinguish them.\n",
    "\n",
    "Another example, is that `\"1998\"` and `\"1999\"` count as distinct types. There are situations where there is no need to distinction between different numbers.\n",
    "\n",
    "The following code performs case normalisation and replaces tokens that consist of digits by \"NUM\". \n",
    "- Python provides a [number of functions](http://docs.python.org/library/stdtypes.html#string-methods), which you can call in order to analyse their content, or produce new strings from them.\n",
    "- The code uses [list comprehension](http://docs.python.org/tutorial/datastructures.html#list-comprehensions) to build a new list by looping through and filtering items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cake', 'is', 'a', 'lie']\n",
      "['in', 'the', 'year', 'NUM', 'of', 'the', 'fourth', 'age', ',', 'after', 'NUM', 'years', 'as', 'king', ',', 'aragorn', 'died', 'at', 'the', 'age', 'of', 'NUM']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"The\",\"cake\",\"is\",\"a\",\"LIE\"]      #a list of tokens, some of which contain uppercase letters\n",
    "print([token.lower() for token in tokens])   #print newly created list of all lowercase tokens\n",
    "\n",
    "numbers = ['in', 'the', 'year', '120', 'of', 'the', 'fourth', 'age', ',', 'after', '120', 'years', 'as', 'king', ',' , 'aragorn', 'died', 'at', 'the', 'age', 'of', '210']\n",
    "print([\"NUM\" if token.isdigit() else token for token in numbers])  #replace all number tokens with \"NUM\" in a new list of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "- Write a function <code>normalise</code> which \n",
    "    * replaces numbers with NUM; \n",
    "    * and replaces tokens such as `\"4th\"`, `\"1st\"` and `\"22nd\"` with `\"Nth\"`.\n",
    "- Test your code on the list `[\"Within\",\"5\",\"minutes\",\",\",\"the\", \"1st\", \"and\", \"2nd\", \"placed\", \"runners\", \"lapped\", \"the\", \"5th\",\".\"]`. \n",
    "- Check that the token `\"and\"` isn't changed to `\"Nth\"`.\n",
    "- You will find [this page](http://docs.python.org/library/stdtypes.html#string-methods) useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Within', 'NUM', 'minutes', ',', 'the', 'Nth', 'and', 'Nth', 'placed', 'runners', 'lapped', 'the', 'Nth', '.']\n"
     ]
    }
   ],
   "source": [
    "def normalise(tokens):\n",
    "    normalisedTokens = []\n",
    "    for token in tokens:\n",
    "        if token.isdigit(): s = \"NUM\"\n",
    "        elif len(token) > 2 and token[-2:] in [\"th\", \"st\", \"nd\"] and token[:-2].isdigit(): s = \"Nth\"\n",
    "        else: s = token\n",
    "        normalisedTokens.append(s)\n",
    "    return normalisedTokens\n",
    "        \n",
    "\n",
    "print(normalise([\"Within\",\"5\",\"minutes\",\",\",\"the\", \"1st\", \"and\", \"2nd\", \"placed\", \"runners\", \"lapped\", \"the\", \"5th\",\".\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "- Complete the code in the cell below. You have just two lines to complete. The goal is to use a large sample of the Reuters corpus to establish the extent to which vocabulary size is reduced when number and case normalisation is applied.\n",
    "- For each of the two incomplete lines you should use nested list comprehensions. This is described in Section 5.1.4 in [this document](http://docs.python.org/tutorial/datastructures.html#list-comprehensions).  Alternatively, you could define functions which iterate over the sentences in each sample and the tokens within each sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalisation produced a 13.21% reduction in vocabulary size from 19253 to 16709\n"
     ]
    }
   ],
   "source": [
    "def vocabulary_size(sentences):\n",
    "    tok_counts = {}\n",
    "    for sentence in sentences: \n",
    "        for token in sentence:\n",
    "            tok_counts[token]=tok_counts.get(token,0)+1\n",
    "    return len(tok_counts.keys())\n",
    "\n",
    "rcr = ReutersCorpusReader()    \n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "raw_sentences = rcr.sample_raw_sents(sample_size)\n",
    "tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n",
    "\n",
    "############################################\n",
    "lowered_sentences = [[token.lower() for token in tokens] for tokens in tokenised_sentences]\n",
    "normalised_sentences = [[\"NUM\" if token.isdigit() else token for token in tokens] for tokens in lowered_sentences]\n",
    "############################################\n",
    "\n",
    "raw_vocab_size = vocabulary_size(tokenised_sentences)\n",
    "normalised_vocab_size = vocabulary_size(normalised_sentences)\n",
    "print(\"Normalisation produced a {0:.2f}% reduction in vocabulary size from {1} to {2}\".format(\n",
    "    100*(raw_vocab_size - normalised_vocab_size)/raw_vocab_size,raw_vocab_size,normalised_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "A considerable amount of the lexical variation found in documents results from the use of morphological variants which we might not wish to distinguish - e.g. when determining the topic of a document. An easy way to remove these varied forms is to use a stemmer. NLTK includes a number of stemmers in the `nltk.stem` package.\n",
    "- [NLTK stem module API](http://nltk.org/api/nltk.stem.html)\n",
    "\n",
    "- [NLTK Porter stemmer](http://nltk.org/api/nltk.stem.html?highlight=stemmer#nltk.stem.porter.PorterStemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at the code below to show how the NLTK implementation of the Porter stemmer in `nltk.stem.porter.PorterStemmer` stems a sample of sentences in the Reuters corpus.\n",
    "- Have a close look at the differences between the columns. This will give you a good indication of what the stemmer does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   BEFORE   AFTER\n",
      "0       (       (\n",
      "1  Approx  approx\n",
      "2       .       .\n",
      "       BEFORE       AFTER\n",
      "0  07/01/1999  07/01/1999\n",
      "1        860M        860m\n",
      "2        6.22        6.22\n",
      "3           %           %\n",
      "       BEFORE   AFTER\n",
      "0        over    over\n",
      "1      German  german\n",
      "2  government  govern\n",
      "3       bonds    bond\n",
      "4           .       .\n",
      "      BEFORE      AFTER\n",
      "0         --         --\n",
      "1    Beijing       beij\n",
      "2   Newsroom   newsroom\n",
      "3          (          (\n",
      "4       8610       8610\n",
      "5          )          )\n",
      "6  6532-1921  6532-1921\n",
      "   BEFORE  AFTER\n",
      "0    1230   1230\n",
      "1     Wed    wed\n",
      "2     CAN    can\n",
      "3     NEW    new\n",
      "4   HOUSE   hous\n",
      "5   PRICE  price\n",
      "6    YAPR   yapr\n",
      "7     N/F    n/f\n",
      "8     PCT    pct\n",
      "9     N/A    n/a\n",
      "10    0.2    0.2\n",
      "         BEFORE    AFTER\n",
      "0            If       If\n",
      "1           the      the\n",
      "2     countries  countri\n",
      "3            of       of\n",
      "4         South    south\n",
      "5       America  america\n",
      "6          want     want\n",
      "7   disarmament  disarma\n",
      "8             ,        ,\n",
      "9          then     then\n",
      "10           do       do\n",
      "11          n't      n't\n",
      "12          buy      buy\n",
      "13         arms      arm\n",
      "14            ,        ,\n",
      "15           ''       ''\n",
      "16           he       he\n",
      "17         told     told\n",
      "18    reporters   report\n",
      "19            .        .\n",
      "  BEFORE AFTER\n",
      "0     ``    ``\n",
      "         BEFORE     AFTER\n",
      "0         North     north\n",
      "1         Korea     korea\n",
      "2           and       and\n",
      "3   independent  independ\n",
      "4       experts    expert\n",
      "5           say       say\n",
      "6           the       the\n",
      "7       country   countri\n",
      "8         needs      need\n",
      "9           2.3       2.3\n",
      "10      million   million\n",
      "11         tons       ton\n",
      "12           of        of\n",
      "13         food      food\n",
      "14         this       thi\n",
      "15         year      year\n",
      "16            .         .\n",
      "        BEFORE      AFTER\n",
      "0          The        the\n",
      "1         City       citi\n",
      "2          has         ha\n",
      "3       agreed       agre\n",
      "4           to         to\n",
      "5         levy       levi\n",
      "6           an         an\n",
      "7           ad         ad\n",
      "8      valorem    valorem\n",
      "9          tax        tax\n",
      "10           ,          ,\n",
      "11      within     within\n",
      "12        such       such\n",
      "13      limits      limit\n",
      "14         now        now\n",
      "15          or         or\n",
      "16   hereafter    hereaft\n",
      "17  prescribed   prescrib\n",
      "18          by         by\n",
      "19         law        law\n",
      "20           ,          ,\n",
      "21          on         on\n",
      "22         all        all\n",
      "23    property   properti\n",
      "24     located      locat\n",
      "25          in         in\n",
      "26         the        the\n",
      "27        City       citi\n",
      "28     subject    subject\n",
      "29          to         to\n",
      "30        such       such\n",
      "31         tax        tax\n",
      "32          as         as\n",
      "33         may        may\n",
      "34          be         be\n",
      "35   necessary  necessari\n",
      "36          to         to\n",
      "37         pay        pay\n",
      "38         the        the\n",
      "39   principal    princip\n",
      "40          of         of\n",
      "41           ,          ,\n",
      "42     premium    premium\n",
      "43           (          (\n",
      "44          if         if\n",
      "45         any        ani\n",
      "46           )          )\n",
      "47         and        and\n",
      "48    interest   interest\n",
      "49          on         on\n",
      "50         the        the\n",
      "51       Bonds       bond\n",
      "52           .          .\n",
      "       BEFORE       AFTER\n",
      "0  06/01/2013  06/01/2013\n",
      "1      1,520M      1,520m\n",
      "2        5.25        5.25\n",
      "3        5.35        5.35\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "rcr = ReutersCorpusReader() \n",
    "st = PorterStemmer()\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "raw_sentences = rcr.sample_raw_sents(sample_size)\n",
    "tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n",
    "\n",
    "for sentence in tokenised_sentences:\n",
    "    df = pd.DataFrame(list(zip_longest(sentence,[st.stem(token) for token in sentence])),columns=[\"BEFORE\",\"AFTER\"])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "- By looking at the impact on a large sample of the Reuters corpus, establish the extent to which vocabulary size is reduced by stemming.\n",
    "- Write code to do this in the empty cell below. You should be able to re-use a lot of the code from the code you used when measuring the impact of lower case and number normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming produced a 26.53% reduction in vocabulary size from 19234 to 14132\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "raw_sentences = rcr.sample_raw_sents(sample_size)\n",
    "tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n",
    "\n",
    "############################################\n",
    "stemmed_sentences = [[st.stem(token) for token in tokens] for tokens in tokenised_sentences]\n",
    "############################################\n",
    "\n",
    "raw_vocab_size = vocabulary_size(tokenised_sentences)\n",
    "stemmed_vocab_size = vocabulary_size(stemmed_sentences)\n",
    "print(\"Stemming produced a {0:.2f}% reduction in vocabulary size from {1} to {2}\".format(\n",
    "    100*(raw_vocab_size - stemmed_vocab_size)/raw_vocab_size,raw_vocab_size,stemmed_vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "* Try using the WordNetLemmatizer <code>nltk.stem.wordnet.WordNetLemmatizer</code> instead of the Porter Stemmer.\n",
    "* Using a large sample of the Reuters corpus, establish the extent to which the vocabulary size reduced by lemmatization?\n",
    "* As an extension, you could look at different sample sizes and/or different corpora and display the results in a table or graph (using <code>pandas</code> and <code>matplotlib</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample size</th>\n",
       "      <th>Reduction in vocab</th>\n",
       "      <th>Raw vocab size</th>\n",
       "      <th>Lemmatized vocab size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>1.93%</td>\n",
       "      <td>828</td>\n",
       "      <td>812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>4.94%</td>\n",
       "      <td>4836</td>\n",
       "      <td>4597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample size Reduction in vocab  Raw vocab size  Lemmatized vocab size\n",
       "0           10              0.00%              99                     99\n",
       "1          100              1.93%             828                    812\n",
       "2         1000              4.94%            4836                   4597"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def getLemReduction(sampleSize):\n",
    "    raw_sentences = rcr.sample_raw_sents(sampleSize)\n",
    "    tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n",
    "\n",
    "    ############################################\n",
    "    lemmatized_sentences = [[lem.lemmatize(token) for token in tokens] for tokens in tokenised_sentences]\n",
    "    ############################################\n",
    "\n",
    "    raw_vocab_size = vocabulary_size(tokenised_sentences)\n",
    "    lemmatized_vocab_size = vocabulary_size(lemmatized_sentences)\n",
    "    \n",
    "    reduction = '{0:.2f}%'.format(100*(raw_vocab_size - lemmatized_vocab_size)/raw_vocab_size)\n",
    "    \n",
    "    return (sampleSize, reduction, raw_vocab_size, lemmatized_vocab_size)\n",
    "\n",
    "cols = []\n",
    "for i in range(1, 4):\n",
    "    cols.append(getLemReduction(10**i))\n",
    "\n",
    "pd.DataFrame(cols, columns=[\"Sample size\", \"Reduction in vocab\", \"Raw vocab size\", \"Lemmatized vocab size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation and stop-word removal\n",
    "A stopword is a word that occurs so often that it loses its usefulness in some tasks. We may get more meaningful information from our corpus analysis if we remove stopwords and punctuation.\n",
    "\n",
    "The code below takes a list of tokens and creates a new list, which contains only those strings which are alphabetic and non-stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', ',', 'which', 'is', 'really', 'fat', ',', 'sat', 'on', 'the', 'mat']\n",
      "['cat', 'really', 'fat', 'sat', 'mat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "tokens=\"The cat , which is really fat , sat on the mat\".lower().split()\n",
    "filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop]\n",
    "print(tokens)\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `isalpha` only returns `True` if the string is entirely composed of alphabet characters. If you want a function to return `True` even when a word contains digits, then you should use `isalnum`.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "- In the empty cell below, write code that looks at a large sample of the Medline corpus, establishing what proportion of tokens are stop-words.\n",
    "- As extension, you could establish the mean (and or the distribution of the) number of stop-words per sentence; or compare the numbers of stop-words in different corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of tokens that are stop-words: 0.32%\n",
      "Mean number of stop words per sentence: 7.73\n"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.corpus_readers import MedlineCorpusReader\n",
    "\n",
    "mcr = MedlineCorpusReader()   \n",
    "\n",
    "sampleSize = 1000\n",
    "\n",
    "raw_sentences = mcr.sample_raw_sents(sampleSize)\n",
    "tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n",
    "\n",
    "stopWordCount = 0\n",
    "tokenCount = 0\n",
    "for tokens in tokenised_sentences:\n",
    "    for token in tokens:\n",
    "        if token in stop: stopWordCount += 1\n",
    "        tokenCount += 1\n",
    "\n",
    "print('Proportion of tokens that are stop-words: {0:.2f}%'.format(stopWordCount / tokenCount))\n",
    "print('Mean number of stop words per sentence: {0:.2f}'.format(stopWordCount / len(tokenised_sentences)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
