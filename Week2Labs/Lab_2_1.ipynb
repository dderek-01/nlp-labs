{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Preprocessing Text (Part 1)\n",
    "\n",
    "Its conventional to group together all of the library imports required at the beginning of the notebook.  You will see these libraries in action later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import zip_longest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "A raw text document is just a sequence of characters. There are a number of basic steps that are often performed when processing natural language text. In lab sessions this week we will cover some of the basic text pre-processing methods. In particular, you will be looking at:\n",
    "- <b> tokenisation</b> - roughly speaking, this involves grouping characters into words;\n",
    "- <b>case normalisation</b> - this involves converting all of the text into lower case; \n",
    "- <b>stemming</b> - this involves removing a word's inflections to find the stem; and \n",
    "- <b>punctuation and stop-word removal</b> - stop-words are common functions words that in some situations can be ignored.\n",
    "\n",
    "Note that we do not always apply all of the above preprocessing methods; it depends on the application. One of the things that you will be learning about in this module, is when the application of each of these methods is, and is not, appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available corpora\n",
    "We have provided simple interfaces to each of the following corpora, which interact well with NLTK tools.\n",
    "\n",
    "- The NLTK texts\n",
    "- Amazon product reviews (~78k documents, ~640k sentences)\n",
    "- Wall Street Journal text (~2k documents, ~51k sentences)\n",
    "- Reuters articles (~61k documents, ~740k sentences)\n",
    "  - Reuters / Finance (~47k documents, ~550k sentences)\n",
    "  - Reuters / Sport (~13k documents, ~185k sentences)\n",
    "- Medline abstracts (~985k documents, ~6100k sentences)\n",
    "- Twitter posts (~962k documents, ~1720k sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access these interfaces, you will need to tell python where your `resources` directory.  This will depend on the machine you are working on.  If you are working on one of the lab machines, it will be available on the teaching drive.  If you are working on your own machine, you will need to download the resources zip file, uncompress it and update the path below accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering') \n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'\\Users\\J\\Desktop\\code\\sussex\\nlp\\labs\\resources')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting raw sentences from a corpus\n",
    "The corpora are too large to easily process with some of the functions you will be using, so we have provided a way for you to work on a randomly selected sample of each corpus.\n",
    "\n",
    "Readers for the Reuters, Twitter and Medline corpora can be initialised using calls to: <code>ReutersCorpusReader()</code>, <code>TwitterCorpusReader()</code> and <code>MedlineCorpusReader()</code> respectively.\n",
    "\n",
    "The readers for each of these corpora have a function called <code style=\"background-color: #F5F5F5;\">sample_raw_sents</code>, which returns a specified number of random sentences, where each sentence is an un-tokenised string.\n",
    "\n",
    "The code in the next cell shows you how to iterate over a random sample of 10 sentences. When you are using a tokeniser, you will replace\n",
    "`# do something with sentence`\n",
    "with code that tokenises each sentence and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex NLTK root directory is \\Users\\J\\Desktop\\code\\sussex\\nlp\\labs\\resources\n",
      "132\n",
      "175\n",
      "2\n",
      "38\n",
      "188\n",
      "41\n",
      "243\n",
      "58\n",
      "19\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "\n",
    "rcr = ReutersCorpusReader()    #Create a new reader\n",
    "\n",
    "sample_size = 10  #size of sample\n",
    "\n",
    "for sentence in rcr.sample_raw_sents(sample_size): #get a sample of random sentences, where each sentence is a string\n",
    "    # do something with sentence\n",
    "    print(len(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "- In the cell below, write code which will print a sample of **20** sentences from the **Twitter** corpus.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @BF_CJed: Hopefully for the Kookaburras Kamran Akmal is the Pakistan keeper #CricketJoke #London2012 \n",
      "\n",
      "All the South African's competing today in the Olympics http://t.co/e8UVogXR \n",
      "\n",
      "The Olympics finish on Sunday :-( \n",
      "\n",
      "RT: Seven Cameroonian athletes, including five boxers, have absconded while in Britain for the Olympics, officials say. \n",
      "\n",
      "Second time at #London2012 that a review on the day's last routine affected medals.  Japan's men won silver after the same thing. \n",
      "\n",
      "#olympics #volleyball @UAAP_VLeague may kahit 1 beses na bang nasali pinas sa vb olympics? haha \n",
      "\n",
      "RT @love_hp_: @teamgb this Olympics is definitely #OurGreatestTeam 20 golds already, what an achievement!! #InspiringANation <3 \n",
      "\n",
      "Incredible numbers of people in Hyde Park today for the Men's Triathlon, proving that you don't need a ticket to feel part of the Olympics \n",
      "\n",
      "http://t.co/I3ID79dn  follow the #Olympics http://t.co/DghUbCW2 \n",
      "\n",
      "If Cats were in the Olympics!! HA! http://t.co/v0dSucvb \n",
      "\n",
      "@ben_kaletskie Mad especially with the olympics on in London!what bout new Orleans?! \n",
      "\n",
      "Wish Iâ€™d charged my phone & bought my earphones with me. Free wifi means I could have watched the #Olympics or any other tv. Balls! \n",
      "\n",
      "this? http://t.co/eTWfip6c  RT @DollFacedBitch: I think the guy running backwards on the Olympics would do better than most. \n",
      "\n",
      "Thanks to @NBCOlympics I already know the outcome of this beach #volleyball game, but I'm still going to watch.  SO THERE. #London2012 \n",
      "\n",
      "i was expecting park jisung to play only to find out he declined the offer to play for the olympics.. interesting.. oh well. \n",
      "\n",
      "omG golden medal in the olympics for algeria in men's 1500M ! proud of my fellow arabs <3 \n",
      "\n",
      "mexico vs brazil for the gold :) #MexFutbol #olympics \n",
      "\n",
      "RT @wheelingprobs: I'm sorry America, I had too. #Olympics http://t.co/CkjyDhA1 \n",
      "\n",
      "Brazil is beating itself #Olympics :) \n",
      "\n",
      "RT @CloydRivers: Winnin' games and takin' names. Merica. Olympics. It's nasty time. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.corpus_readers import TwitterCorpusReader\n",
    "\n",
    "tcr = TwitterCorpusReader()    #Create a new reader\n",
    "\n",
    "sampleSize = 20\n",
    "\n",
    "for sentence in tcr.sample_raw_sents(sampleSize):\n",
    "    print(sentence, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, point your browser at [Sussex NLTK package documentation](http://www.sussex.ac.uk/Users/davidw/courses/nle/SussexNLTK-API/) and have a look around. This provides information about the above corpora. Take a particularly careful look at the [corpus_readers Module](http://www.sussex.ac.uk/Users/davidw/courses/nle/SussexNLTK-API/sussex_nltk.html#module-sussex_nltk.corpus_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "- In the cell below write code that will take a sample of sentences from each of the Reuters, Twitter and Medline corpora and displays the average length (as measured in terms of the number of characters in the sentence).\n",
    "\n",
    "- How large do you think the sample size should be in order to decide whether the differences observed are systematic or random? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.2\n",
      "100.23333333333333\n",
      "151.63333333333333\n"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.corpus_readers import MedlineCorpusReader\n",
    "\n",
    "mcr = MedlineCorpusReader()    #Create a new reader\n",
    "\n",
    "sampleSize = 30\n",
    "\n",
    "def printAvgLength(r):\n",
    "    totalLength = 0\n",
    "    for sentence in r.sample_raw_sents(sampleSize):\n",
    "        totalLength += len(sentence)\n",
    "    print(totalLength / sampleSize)\n",
    "\n",
    "printAvgLength(rcr)\n",
    "printAvgLength(tcr)\n",
    "printAvgLength(mcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sample size should be large enough such that the average length doesn't deviate too much with each run of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY Tokenisation with Regular Expressions\n",
    "Text doesn't come in neat tokens ready for analysis, it must first undergo sentence segmentation and tokenisation.  \n",
    "We have already sentence segmented the corpora.  \n",
    "In this lab you will be focusing on tokenisation, in particular, you will be comparing the merits of the following tokenisers:  \n",
    "- Your own regular expression based tokeniser\n",
    "- The (NLTK implemented) PENN treebank style regular expression based tokeniser\n",
    "- A Twitter-specific CMU tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to consider\n",
    "Your goal when working through this next section should be to investigate the strengths and weaknesses of each of the 3 tokenisers on three rather different kinds of corpora: \n",
    "- the Reuters corpus, \n",
    "- the Twitter corpus and \n",
    "- the Medline corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making your own tokeniser\n",
    "In this section, you will write your own Python function, which takes as input a single string representing a sentence, and returns a <b>list of strings</b> obtained by splitting the sentence into tokens.\n",
    "\n",
    "Let's start by simply splitting by whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"   What    is the    air-speed   velocity of  an unladen swallow?   \".split()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "- In the empty code cell below write a [function](http://docs.python.org/tutorial/controlflow.html#defining-functions), `tokenise` which takes a sentence as input and returns a list of the tokens making up the sentence. Your first version of this function should tokenise only on whitespace, as shown in the cell above. Show that your function works on the sentence shown above.\n",
    "- Note: this is intended to be an easy exercise - just a couple of lines of code - don't overcomplicate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'the', 'air-speed', 'velocity', 'of', 'an', 'unladen', 'swallow?']\n"
     ]
    }
   ],
   "source": [
    "def tokenise(s):\n",
    "    return s.split()\n",
    "\n",
    "print(tokenise(\"   What    is the    air-speed   velocity of  an unladen swallow?   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "- In the empty code cell below write code that applies your tokenise function to each sentence in a sample of 30 sentences taken from  the Reuters, Twitter and Medline corpora, 10 sentences from each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DTD:', '07/15/97', 'SURE', 'BID:', 'N']\n",
      "['Brazil', 'draws', '$6.0', 'bln', 'in', 'investment', 'January-May.']\n",
      "['1030', 'Fri', 'UK', 'CBI', 'MONTHLY', 'TRENJUN', 'N/F', 'N/A', 'N/A']\n",
      "['The', 'Bank', 'has', 'long', 'maintained', 'that', 'to', 'regard', 'a', 'strong', 'currency', 'as', 'a', 'de', 'facto', 'monetary', 'tightening', 'is', 'dangerous.']\n",
      "['((--', '212-859-1660', '))']\n",
      "['Unemployment', 'in', \"China's\", 'crowded', 'cities', 'is', 'growing', 'and', 'the', 'gap', 'between', 'the', 'urban', 'rich', 'and', 'poor', 'is', 'widening', 'sharply,', 'state', 'media', 'has', 'said.']\n",
      "['The', 'monthly', 'inflation', 'rate', 'in', 'June', 'was', '1.1', 'percent.']\n",
      "['01/01/2011', '626207ED2']\n",
      "['\"It', 'is', 'up', 'to', 'us', 'to', 'determine', 'if', 'we', 'want', 'to', 'continue', 'to', 'conduct', 'our', 'autonomous', 'monetary', 'policy', 'or', 'adjust', 'to', 'the', 'euro.']\n",
      "['The', 'final', '$12.569', 'billion', 'budget', 'was', 'raised', 'from', 'the', '$11.976', 'billion', 'spending', 'plan', 'proposed', 'in', 'April', 'by', 'Los', 'Angeles', \"County's\", 'chief', 'administrative', 'officer.']\n",
      "['Gold', 'silver', 'and', 'bronze', 'goes', 'to', 'blake', 'weir', 'and', 'bolt', 'certified', '#Olympics', '#200M']\n",
      "['I', 'had', 'a', 'dream', 'I', 'was', 'in', 'the', 'Olympics', 'lol.']\n",
      "['@dwaynewade', 'we', 'miss', 'you', 'shooting', 'hoops', 'man!!i', 'wish', 'you', 'where', 'at', 'the', 'olympics..']\n",
      "['RT', '@KlintTheDrunk:', 'Yoruba', 'man', 'go', 'remove', 'clothes,', 'wrist', 'watch', 'and', 'pant', 'to', 'fight', '4', 'Ojuelegba.', 'See', 'boxing', '4', 'olympics,we', 'no', 'see', 'dem']\n",
      "['RT', '@YolantheCabau:', 'Epke', 'Zonderland', 'gefeliciteerd!!', 'Geweldige', 'prestatie!!', 'Gold', 'for', 'Holland', 'again!!', '#Olympics']\n",
      "['Hopefully', 'Pendleton', 'adds', 'another', 'gold', 'medal', 'to', 'the', 'collection..#TeamGB']\n",
      "['Hi', '@Olympics', '@fifa', 'can', 'you', 'confirm', 'how', 'much', 'money', 'referee', 'Christina', 'Pedersen', 'got', 'paid', 'to', 'gift', 'the', 'US', 'that', 'soccer', 'win', 'yesterday?', '#corruption']\n",
      "['GOALLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL!!!!!!!!!!!!!!', '3-1', \"Mexico's\", 'in', 'the', 'finals!', 'Si', 'se', 'pudo!!', '#Olympics']\n",
      "['@piersmorgan', 'itching', 'to', 'give', 'another', '1k', 'to', 'GOSH?', 'From', 'all', 'indications', '#teamGB', 'might', 'break', 'ur', 'bank..#keepitup']\n",
      "['RT', '@AndrewFstewart:', 'RT', '@casiestewart:', '10', 'eye-popping', '#Olympics', 'photos', 'via', '@Storify', 'http://t.co/se9ki6Hw']\n",
      "['Dynamic', 'transverse', 'axial', 'wall', 'tomograms', 'of', 'the', 'left', 'ventricle', '(LV)', 'are', 'reconstructed', 'by', 'a', 'new', 'technique', 'from', 'anterior', 'and', 'LAO', 'views', 'acquired', 'with', 'a', 'conventional', 'scintillation', 'camera', 'imaging', 'the', 'distribution', 'of', 'in-vivo', 'Tc-99m-labeled', 'red', 'blood', 'cells.']\n",
      "['aorta', 'or', 'umbilical', 'vein,', 'was', 'identified', 'in', 'the', 'real-time', 'image', 'and', 'the', 'real-time', 'transducer', 'put', 'parallel', 'to', 'the', 'vessel.']\n",
      "['Section', '2', 'discusses', 'his', 'development', 'of', 'the', 'odds', 'ratio', 'obtained', 'in', 'a', 'case-control', 'study', 'as', 'an', 'estimate', 'of', 'the', 'relative', 'risk', 'of', 'the', 'disease', 'under', 'study.']\n",
      "['83,', '441-448),', 'it', 'was', 'found', 'that,', 'in', 'the', 'absence', 'of', 'ascorbate,', 'prolyl', '4-hydroxylase', '(prolyl-glycyl-peptide,', '2-oxoglutarate:oxygen', 'oxidoreductase', '(4-hydroxylating),', 'EC', '1.14.11.2)', 'catalyses', 'the', 'hydroxylation', 'of', 'peptidyl', 'proline,', 'stoicheiometrically', 'coupled', 'to', 'the', 'oxidative', 'decarboxylation', 'of', '2-oxoglutarate,', 'at', 'a', 'high', 'initial', 'rate.']\n",
      "['The', 'findings', 'of', 'this', 'study', 'revealed', 'that', 'little', 'difference', 'existed', 'between', 'the', 'ventilatory', 'values', 'of', 'children', 'tested', 'in', 'Perth,', 'Europe', 'and', 'the', 'Eastern', 'States', 'of', 'Australia.']\n",
      "['It', 'should', 'only', 'be', 'used', 'under', 'strict', 'veterinary', 'control', 'and', 'then', 'only', 'if', 'there', 'are', 'clear', 'clinical', 'indications.']\n",
      "['A', 'persistent', 'infection', 'by', 'a', 'baculovirus-like', 'particle', 'was', 'found', 'in', 'the', 'established', 'lepidopteran', '(Heliothis', 'zea)', 'cell', 'line,', 'IMC-HZ-1.']\n",
      "['This', 'reduction', 'was', 'unaltered', 'by', 'either', 'Mg2+', '(30', 'mM)', 'or', 'La3+', '(0.1', 'mM),', 'prevented', 'by', 'either', 'Sr2+', '(30', 'mM)', 'or', 'Ca2+', '(1', 'mM)', 'and', 'augmented', 'by', 'Ba2+', '(30', 'mM)', 'which', 'also', 'markedly', 'increased', 'spontaneous', 'histamine', 'release', 'during', 'pre-incubation', 'at', '37', 'degrees', 'C.', 'Results', 'suggest', 'that', 'Ba2+', 'and', 'Sr2+', 'interact', 'with', 'cell-fixed', 'calcium', 'to', 'modulate', 'histamine', 'release', 'by', 'compound', '48/80.']\n",
      "['Matsushita', 'resistors', 'have', 'a', 'much', 'smaller', 'heat', 'capacity', 'than', 'Speer', 'resistors,', 'and', 'thus', 'are', 'preferable', 'in', 'some', 'thermometry', 'applications', 'at', 'temperatures', 'below', '1', 'K.']\n",
      "['5', 'in', 'whom', 'a', 'clinical', 'and', 'histological', 'diagnosis', 'of', 'Indian', 'Childhood', 'Cirrhosis', 'was', 'made', 'had', 'massive', 'orcein-staining', 'deposits', 'in', 'liver', 'cells.']\n"
     ]
    }
   ],
   "source": [
    "sampleSize = 10\n",
    "\n",
    "def tokeniseSamples(r):\n",
    "    for sentence in r.sample_raw_sents(sampleSize):\n",
    "        print(tokenise(sentence))\n",
    "        \n",
    "tokeniseSamples(rcr)\n",
    "tokeniseSamples(tcr)\n",
    "tokeniseSamples(mcr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most tokenisation policies (e.g. in the Wall Street Journal corpus), contractions like \"I'm\" tend to be split into \"I\" and \"'m\".  \n",
    "\n",
    "When it comes to more than just splitting by whitespace, it can be convenient to use [regular expressions](http://docs.python.org/library/re.html) to process the string in some way. The following code cell illustrates this. Trying running it and then read on to discover how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(\"([.?!'])\", \" \\g<1>\", \"You're using coconuts!\").split())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the above code works by breaking it down.  \n",
    "\n",
    "First, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(re.sub(\"'\", \" '\", \"You're using coconuts!\")   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this code takes the string \"You're using coconuts!\" and inserts a space before the apostophe, the `'` character. \n",
    "\n",
    "Let's see how it works...\n",
    "\n",
    "The first argument of `re.sub`, i.e. `\"'\"`, is a regular expression that in this case is extremely simple, since it only matches the apostophe character, `'`.\n",
    "\n",
    "The second argument of `re.sub`, where we see `\" '\"`, indicates that an apostophe should be substituted by a space followed by an apostophe.\n",
    "\n",
    "Now let's make it slightly more complicated. We also want to insert a space before the `\"!\"`, so let's look at how to do that. \n",
    "\n",
    "Run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(\"(['!])\", \" \\g<1>\", \"You're using coconuts!\")   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument of `re.sub`, has been changed to `\"(['!])\"`, which is a regular expression that matches either an apostophe character,`'`, or an exclamation mark,`!`.\n",
    "\n",
    "This is achieved with the regular expression `\"['!]\"`, where the square brackets enclose the alternative characters. \n",
    "\n",
    "Why does the regular expression contain parenthesis? \n",
    "\n",
    "It has to do with what we need to put as the second argument of `re.sub` where the substitution is specified. \n",
    "\n",
    "To understand this, you need to appreciate that we want to add a space before an apostrophe and also a space before an exclamation mark. How can we specify that in the second argument of `re.sub`? \n",
    "\n",
    "The answer is that we need to make use of the the idea of a **group**.\n",
    "\n",
    "The parenthesis in `\"(['!])\"` define the start and end of a group. In this case the whole regular expression is a group. In general, however, there can be several sets of parentheis defining several groups. For example, the regular expression `\"([Tt]h)e (m*n)\"` has two groups. Groups are numbered from left to right, so the group in the regular expression `\"(['!])\"` is group 1. \n",
    "\n",
    "Defining this group allows us to refer to the string that matches the regular expression `\"(['!])\"`, which will be either an apostrophe or an exclamation mark. This is then used in the second argument of `re.sub`, where we see `\" \\g<1>\"`, which indicates that the material that matches the apostophe or exclamation mark should be substituted by a space followed by the symbol that was matched. The `1` in `\\g<1>` tells us that it is group one.\n",
    "\n",
    "We are now ready to look at the original code, which is reproduced below and should now make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"'re\", 'using', 'coconuts', '!']\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"([.?!'])\", \" \\g<1>\", \"You're using coconuts!\").split())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the spaces are added before any full stop, question mark, exclamation mark or apostrophe.\n",
    "The resulting string is then split on white space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "- Write a new version of your `tokenise` function that uses `re.sub` in the way we've just considered. Make sure you test your new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"'re\", 'using', 'coconuts', '!']\n"
     ]
    }
   ],
   "source": [
    "def tokenise(s):\n",
    "    return re.sub(\"([.?!'])\", \" \\g<1>\", s).split()\n",
    "\n",
    "print(tokenise(\"You're using coconuts!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "\n",
    "\n",
    "- In an empty code cell below, extend your tokeniser function to cater for the following guidelines. \n",
    "- Test out your new tokeniser on the string  \n",
    "`\"After saying \\\"I won't help, I'm gonna leave!\\\", on his parents' arrival, the boy's behaviour improved.\"`  \n",
    " notice that the `\"` characters in the test sentence have been espaced, appearing as `\\\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "- punctuation is split from adjoining words\n",
    "- opening double quotes are changed to two single forward quotes.\n",
    "- closing double quotes are changed to two single backward quotes.\n",
    "- the Anglo-Saxon genitive of nouns are split into their component morphemes, and each morpheme is tagged separately.\n",
    "  - e.g. `\"children's\"` produces `\"children 's\"`\n",
    "  - e.g. `\"parents'\"` produces `\"parents '\"`\n",
    "- contractions should be split into component morphenes\n",
    "  - e.g. `\"won't\"` produces `\"wo n't\"`\n",
    "  - e.g. `\"gonna\"` produces `\"gon na\"`\n",
    "  - e.g. `\"I'm\"` produces `\"I 'm\"`\n",
    "  \n",
    "  \n",
    "These tokenisation guidelines are a subset of those found at\n",
    "ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenization.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hints:\n",
    "\n",
    "- Use multiple calls to `re.sub` to deal with different cases one at a time. As in...\n",
    "\n",
    "```\n",
    "    sentence = re.sub(<pattern1>, <replacement1>,sentence)\n",
    "    sentence = re.sub(<pattern2>, <replacement2>,sentence)\n",
    "    sentence = re.sub(<pattern3>, <replacement3>,sentence)\n",
    "```\n",
    "\n",
    "- Order your calls to `re.sub` so that you deal with the specific cases first and the more general cases later.\n",
    "\n",
    "- In dealing with the replacement of start and end `\"`, you will find the following useful:\n",
    "\n",
    ">The `'*'`, `'+'`, and `'?'` qualifiers are all *greedy*; they match\n",
    ">as much text as possible.  Sometimes this behaviour isn't desired; if the RE\n",
    ">`<.\\*>` is matched against `<a> b <c>`, it will match the entire\n",
    ">string, and not just `<a>`.  Adding `'?'` after the qualifier makes it\n",
    ">perform the match in *non-greedy* or *minimal* fashion; as *few*\n",
    ">characters as possible will be matched.  Using the RE `<.\\*?>` will match\n",
    ">only `<a>`.  \n",
    "(taken from https://docs.python.org/2/library/re.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'saying', '``', 'I', 'wo', \"n't\", 'help', ',', 'I', \"'m\", 'gon', 'na', 'leave', '!', 'Â´Â´', ',', 'on', 'his', 'parents', \"'\", 'arrival', ',', 'the', 'boy', \"'s\", 'behaviour', 'improved', '.']\n"
     ]
    }
   ],
   "source": [
    "def tokenise(s):\n",
    "    s = re.sub(\"(.*n)(na)\", \"\\g<1> \\g<2>\", s)\n",
    "    s = re.sub(\"'s\", \" 's\", s)\n",
    "    s = re.sub(\"s'\", \"s '\", s)\n",
    "    s = re.sub(\"n't\", \" n't\", s)\n",
    "    s = re.sub(\"'m\", \" 'm\", s)\n",
    "    s = re.sub(\"([.?!:;,])\", \" \\g<1>\", s)\n",
    "    s = re.sub(\"(\\\")(.*?\\\")\", \"`` \\g<2>\", s)\n",
    "    s = re.sub(\"(``.*?)(\\\")\", r\"\\g<1> Â´Â´\", s)\n",
    "    return s.split()\n",
    "\n",
    "print(tokenise(\"After saying \\\"I won't help, I'm gonna leave!\\\", on his parents' arrival, the boy's behaviour improved.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLTK regular expression tokeniser\n",
    "The NLTK implements a regular expression tokeniser `word_tokenize` that is based on the above tokenisation guidelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: `word_tokenize`\n",
    "\n",
    "- Arguments\n",
    " - a single string, representing a sentence\n",
    "- Returns\n",
    " - a list of strings, where each string is a token within the sentence</dd>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- Make sure you understand the code in the cell below and then run it so that you can compare the way that the test sentence has been tokenised by the two tokenisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK</th>\n",
       "      <th>MINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After</td>\n",
       "      <td>After</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saying</td>\n",
       "      <td>saying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>``</td>\n",
       "      <td>``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wo</td>\n",
       "      <td>wo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n't</td>\n",
       "      <td>n't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'m</td>\n",
       "      <td>'m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gon</td>\n",
       "      <td>gon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>leave</td>\n",
       "      <td>leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>``</td>\n",
       "      <td>Â´Â´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>his</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>parents</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>arrival</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>boy</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>'s</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>behaviour</td>\n",
       "      <td>behaviour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>improved</td>\n",
       "      <td>improved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         NLTK       MINE\n",
       "0       After      After\n",
       "1      saying     saying\n",
       "2          ``         ``\n",
       "3           I          I\n",
       "4          wo         wo\n",
       "5         n't        n't\n",
       "6        help       help\n",
       "7           ,          ,\n",
       "8           I          I\n",
       "9          'm         'm\n",
       "10        gon        gon\n",
       "11         na         na\n",
       "12      leave      leave\n",
       "13          !          !\n",
       "14         ``         Â´Â´\n",
       "15          ,          ,\n",
       "16         on         on\n",
       "17        his        his\n",
       "18    parents    parents\n",
       "19          '          '\n",
       "20    arrival    arrival\n",
       "21          ,          ,\n",
       "22        the        the\n",
       "23        boy        boy\n",
       "24         's         's\n",
       "25  behaviour  behaviour\n",
       "26   improved   improved\n",
       "27          .          ."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "    \n",
    "testsentence = \"After saying \\\"I won't help, I'm gonna leave!\\\", on his parents' arrival, the boy's behaviour improved.\"\n",
    "\n",
    "# run the nltk tokeniser and your tokeniser on the test sentence\n",
    "nltk_toks = word_tokenize(testsentence) # run the nltk tokeniser\n",
    "my_toks = tokenise(testsentence) # run your tokeniser\n",
    "\n",
    "pd.DataFrame(list(zip_longest(nltk_toks,my_toks)),columns=[\"NLTK\", \"MINE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the word_tokenize function from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# run the nltk tokeniser on a test sentence\n",
    "test_sentence=\"The cat sat on the mat.\"\n",
    "word_tokenize(test_sentence) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Exercise 3.2\n",
    "\n",
    "- In the code cell below write code to run both the `NLTK_Tokenise` and your own `Tokenise` function on a sample of 10 sentences from the Reuters corpus.\n",
    "- Look for differences in the output of the two tokenisers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sept. 4\n",
      "['Sept.', '4']\n",
      "['Sept', '.', '4']\n",
      "\"The euro is a very much more attractive investment currency than the mark, purely in terms of its liquidity, and will be a seriously more attractive invoicing currency for world trade.\"\n",
      "['``', 'The', 'euro', 'is', 'a', 'very', 'much', 'more', 'attractive', 'investment', 'currency', 'than', 'the', 'mark', ',', 'purely', 'in', 'terms', 'of', 'its', 'liquidity', ',', 'and', 'will', 'be', 'a', 'seriously', 'more', 'attractive', 'invoicing', 'currency', 'for', 'world', 'trade', '.', \"''\"]\n",
      "['``', 'The', 'euro', 'is', 'a', 'very', 'much', 'more', 'attractive', 'investment', 'currency', 'than', 'the', 'mark', ',', 'purely', 'in', 'terms', 'of', 'its', 'liquidity', ',', 'and', 'will', 'be', 'a', 'seriously', 'more', 'attractive', 'invoicing', 'currency', 'for', 'world', 'trade', '.', 'Â´Â´']\n",
      "  pct change month/month\t   0.7\t  0.1\t 2.6\n",
      "['pct', 'change', 'month/month', '0.7', '0.1', '2.6']\n",
      "['pct', 'change', 'month/month', '0', '.7', '0', '.1', '2', '.6']\n",
      "The Central Bank is scheduled to cut banks' reserve requirement by another one percentage point to 13 percent beginning July 4.\n",
      "['The', 'Central', 'Bank', 'is', 'scheduled', 'to', 'cut', 'banks', \"'\", 'reserve', 'requirement', 'by', 'another', 'one', 'percentage', 'point', 'to', '13', 'percent', 'beginning', 'July', '4', '.']\n",
      "['The', 'Central', 'Bank', 'is', 'scheduled', 'to', 'cut', 'banks', \"'\", 'reserve', 'requirement', 'by', 'another', 'one', 'percentage', 'point', 'to', '13', 'percent', 'beginning', 'July', '4', '.']\n",
      "--Lisbon bureau 3511-3150035\n",
      "['--', 'Lisbon', 'bureau', '3511-3150035']\n",
      "['--Lisbon', 'bureau', '3511-3150035']\n",
      " 0830  Mon   UK  M4 (FINAL)(Y/Y) APR   N/F  PCT    N/A\tN/A\n",
      "['0830', 'Mon', 'UK', 'M4', '(', 'FINAL', ')', '(', 'Y/Y', ')', 'APR', 'N/F', 'PCT', 'N/A', 'N/A']\n",
      "['0830', 'Mon', 'UK', 'M4', '(FINAL)(Y/Y)', 'APR', 'N/F', 'PCT', 'N/A', 'N/A']\n",
      "school improvement bonds.\n",
      "['school', 'improvement', 'bonds', '.']\n",
      "['school', 'improvement', 'bonds', '.']\n",
      "\t\t\t\t\t\t  (Approx.\n",
      "['(', 'Approx', '.']\n",
      "['(Approx', '.']\n",
      "The senior European Union negotiator at the bilateral talks on meat import inspection regulations predicted \"tough discussions\" ahead in the next few days as negotiators make a last-ditch effort to ward off a trade war.\n",
      "['The', 'senior', 'European', 'Union', 'negotiator', 'at', 'the', 'bilateral', 'talks', 'on', 'meat', 'import', 'inspection', 'regulations', 'predicted', '``', 'tough', 'discussions', \"''\", 'ahead', 'in', 'the', 'next', 'few', 'days', 'as', 'negotiators', 'make', 'a', 'last-ditch', 'effort', 'to', 'ward', 'off', 'a', 'trade', 'war', '.']\n",
      "['The', 'senior', 'European', 'Union', 'negotiator', 'at', 'the', 'bilateral', 'talks', 'on', 'meat', 'import', 'inspection', 'regulations', 'predicted', '``', 'tough', 'discussions', 'Â´Â´', 'ahead', 'in', 'the', 'next', 'few', 'days', 'as', 'negotiators', 'make', 'a', 'last-ditch', 'effort', 'to', 'ward', 'off', 'a', 'trade', 'war', '.']\n",
      "\t     MATURITY\t\t\t     CUSIP\n",
      "['MATURITY', 'CUSIP']\n",
      "['MATURITY', 'CUSIP']\n"
     ]
    }
   ],
   "source": [
    "sampleSize = 10\n",
    "\n",
    "sampleSentences = rcr.sample_raw_sents(sampleSize)\n",
    "\n",
    "for sentence in sampleSentences:\n",
    "    print(sentence)\n",
    "    print(word_tokenize(sentence))\n",
    "    print(tokenise(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Twitter-specific Tokeniser\n",
    "The third tokeniser for you to explore is a Twitter-specific tokeniser that has been developed by [Gimpel et al.](http://ttic.uchicago.edu/~kgimpel/papers/gimpel+etal.acl11.pdf).\n",
    "\n",
    "These functions are located in the <code>sussex_nltk.tokenize</code> module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Function**: `twitter_tokenize`\n",
    "- Arguments\n",
    " - a single string, representing a sentence\n",
    "- Returns\n",
    " - a list of strings, where each string is a token within the sentence\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`twitter_tokenize` can be quite slow, so we have provided the following function to tokenise an entire sample of sentences at once.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Function**: `twitter_tokenize_batch`\n",
    "- Arguments\n",
    " - a list of strings, where each string represents a sentence\n",
    "- Returns\n",
    " - a list of sentences, where each sentence is a list of tokens\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Java Error!!!\n",
    "\n",
    "Depending on the machine you are working on, the twitter_tokenizer may fail because it cannot find the JRE.  If this happens to you, either on a lab machine or your own machine, you will need to locate the java executable, and then update and run the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.internals.config_java(\"C:/Program Files (x86)/Common Files/Oracle/Java/javapath/java.exe\",options='-Xmx1g -XX:ParallelGCThreads=2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided the Java executable can be found, the following cell should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'saying', '\"', 'I', \"won't\", 'help', ',', \"I'm\", 'gonna', 'leave', '!', '\"', ',', 'on', 'his', 'parents', \"'\", 'arrival', ',', 'the', \"boy's\", 'behaviour', 'improved', '.']\n"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.tokenize import twitter_tokenize\n",
    "twitter_toks = twitter_tokenize(testsentence)\n",
    "print(twitter_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "- In the empty cell below, write code to run both  `twitter_tokenize` and the NLTK tokeniser, `word_tokenize`, function on each sentence in a sample of 10 sentences from the Twitter corpus.\n",
    "- Display each sentence tokenised by the two tokenisers. \n",
    "- Once you have done this, look for differences in the output of the two tokenisers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#teamgb Watch free sex shows with our girls www freecamsmax com\n",
      "['#teamgb', 'Watch', 'free', 'sex', 'shows', 'with', 'our', 'girls', 'www', 'freecamsmax', 'com']\n",
      "['#', 'teamgb', 'Watch', 'free', 'sex', 'shows', 'with', 'our', 'girls', 'www', 'freecamsmax', 'com'] \n",
      "\n",
      "RT @LawsonOfficial: Performing at the Olympics today! What an honour. #rockthegames http://t.co/Gb1KOiwx\n",
      "['RT', '@LawsonOfficial', ':', 'Performing', 'at', 'the', 'Olympics', 'today', '!', 'What', 'an', 'honour', '.', '#rockthegames', 'http://t.co/Gb1KOiwx']\n",
      "['RT', '@', 'LawsonOfficial', ':', 'Performing', 'at', 'the', 'Olympics', 'today', '!', 'What', 'an', 'honour', '.', '#', 'rockthegames', 'http', ':', '//t.co/Gb1KOiwx'] \n",
      "\n",
      "Sheer dominance in the triathlon! #19 #GOLD #teamGB\n",
      "['Sheer', 'dominance', 'in', 'the', 'triathlon', '!', '#19', '#GOLD', '#teamGB']\n",
      "['Sheer', 'dominance', 'in', 'the', 'triathlon', '!', '#', '19', '#', 'GOLD', '#', 'teamGB'] \n",
      "\n",
      "Congratulations to #TeamGB. Medal tally now guaranteed to beat the Beijing medals. Amazing work.\n",
      "['Congratulations', 'to', '#TeamGB', '.', 'Medal', 'tally', 'now', 'guaranteed', 'to', 'beat', 'the', 'Beijing', 'medals', '.', 'Amazing', 'work', '.']\n",
      "['Congratulations', 'to', '#', 'TeamGB', '.', 'Medal', 'tally', 'now', 'guaranteed', 'to', 'beat', 'the', 'Beijing', 'medals', '.', 'Amazing', 'work', '.'] \n",
      "\n",
      "#olympics Free 1000 tv channels on your phone? http://t.co/rEh5bIDT http://t.co/jHWPZBT5 fufu\n",
      "['#olympics', 'Free', '1000', 'tv', 'channels', 'on', 'your', 'phone', '?', 'http://t.co/rEh5bIDT', 'http://t.co/jHWPZBT5', 'fufu']\n",
      "['#', 'olympics', 'Free', '1000', 'tv', 'channels', 'on', 'your', 'phone', '?', 'http', ':', '//t.co/rEh5bIDT', 'http', ':', '//t.co/jHWPZBT5', 'fufu'] \n",
      "\n",
      "The greatest EVER!!! Sir Chris Hoy!! Wow!!! #TeamGB #velodrome #GOLD\n",
      "['The', 'greatest', 'EVER', '!!!', 'Sir', 'Chris', 'Hoy', '!!', 'Wow', '!!!', '#TeamGB', '#velodrome', '#GOLD']\n",
      "['The', 'greatest', 'EVER', '!', '!', '!', 'Sir', 'Chris', 'Hoy', '!', '!', 'Wow', '!', '!', '!', '#', 'TeamGB', '#', 'velodrome', '#', 'GOLD'] \n",
      "\n",
      "YES!!! @chrishoy 6 time Olympic champion!!!! #TeamGB #olympics2012\n",
      "['YES', '!!!', '@chrishoy', '6', 'time', 'Olympic', 'champion', '!!!!', '#TeamGB', '#olympics2012']\n",
      "['YES', '!', '!', '!', '@', 'chrishoy', '6', 'time', 'Olympic', 'champion', '!', '!', '!', '!', '#', 'TeamGB', '#', 'olympics2012'] \n",
      "\n",
      "Queen Vic signs off with swansong silver http://t.co/JLmOOpdk #London2012\n",
      "['Queen', 'Vic', 'signs', 'off', 'with', 'swansong', 'silver', 'http://t.co/JLmOOpdk', '#London2012']\n",
      "['Queen', 'Vic', 'signs', 'off', 'with', 'swansong', 'silver', 'http', ':', '//t.co/JLmOOpdk', '#', 'London2012'] \n",
      "\n",
      "Being in the olympics for floor hockey would be sick\n",
      "['Being', 'in', 'the', 'olympics', 'for', 'floor', 'hockey', 'would', 'be', 'sick']\n",
      "['Being', 'in', 'the', 'olympics', 'for', 'floor', 'hockey', 'would', 'be', 'sick'] \n",
      "\n",
      "another medal from our boxers! yeaow! well done to Conlan! #teamireland #london2012\n",
      "['another', 'medal', 'from', 'our', 'boxers', '!', 'yeaow', '!', 'well', 'done', 'to', 'Conlan', '!', '#teamireland', '#london2012']\n",
      "['another', 'medal', 'from', 'our', 'boxers', '!', 'yeaow', '!', 'well', 'done', 'to', 'Conlan', '!', '#', 'teamireland', '#', 'london2012'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleSize = 10\n",
    "\n",
    "sampleSentences = tcr.sample_raw_sents(sampleSize)\n",
    "\n",
    "for sentence in sampleSentences:\n",
    "    print(sentence)\n",
    "    print(twitter_tokenize(sentence))\n",
    "    print(word_tokenize(sentence), '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2\n",
    "- Copy the code cell above and move the copy to below this cell. Then use both the NLTK and Twitter tokenisers on a sample of 10 sentences from the **Medline** corpus.\n",
    "- Look for situations where the  tokenisers do not tokenise appropriately.\n",
    "- Try to figure out the differences in tokenisation policies of the tokenisers.\n",
    "- Think about possible motivations for the differences in tokenisation policy, by considering how the tokens may be used in subsequent (down-stream) language processing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixteen types of virulent phages active on Rps.\n",
      "['Sixteen', 'types', 'of', 'virulent', 'phages', 'active', 'on', 'Rps', '.']\n",
      "['Sixteen', 'types', 'of', 'virulent', 'phages', 'active', 'on', 'Rps', '.'] \n",
      "\n",
      "Polymerizing G actin in the presence of spectrin tetramers or mixing preformed F actin with spectrin tetramer plus bad 4.1 results in an extensively crosslinked network of actin filaments.\n",
      "['Polymerizing', 'G', 'actin', 'in', 'the', 'presence', 'of', 'spectrin', 'tetramers', 'or', 'mixing', 'preformed', 'F', 'actin', 'with', 'spectrin', 'tetramer', 'plus', 'bad', '4.1', 'results', 'in', 'an', 'extensively', 'crosslinked', 'network', 'of', 'actin', 'filaments', '.']\n",
      "['Polymerizing', 'G', 'actin', 'in', 'the', 'presence', 'of', 'spectrin', 'tetramers', 'or', 'mixing', 'preformed', 'F', 'actin', 'with', 'spectrin', 'tetramer', 'plus', 'bad', '4.1', 'results', 'in', 'an', 'extensively', 'crosslinked', 'network', 'of', 'actin', 'filaments', '.'] \n",
      "\n",
      "Classical psychosomatic studies with a justified claim to be of scientific value, have not changed this situation.\n",
      "['Classical', 'psychosomatic', 'studies', 'with', 'a', 'justified', 'claim', 'to', 'be', 'of', 'scientific', 'value', ',', 'have', 'not', 'changed', 'this', 'situation', '.']\n",
      "['Classical', 'psychosomatic', 'studies', 'with', 'a', 'justified', 'claim', 'to', 'be', 'of', 'scientific', 'value', ',', 'have', 'not', 'changed', 'this', 'situation', '.'] \n",
      "\n",
      "These results are largely confirmed by quantitative absorptions, except that no diminution of H-2k expression is observed, suggesting that fusion with the TerC parents may transfer a resistance to complement damage to the L-cell parent.\n",
      "['These', 'results', 'are', 'largely', 'confirmed', 'by', 'quantitative', 'absorptions', ',', 'except', 'that', 'no', 'diminution', 'of', 'H-2k', 'expression', 'is', 'observed', ',', 'suggesting', 'that', 'fusion', 'with', 'the', 'TerC', 'parents', 'may', 'transfer', 'a', 'resistance', 'to', 'complement', 'damage', 'to', 'the', 'L-cell', 'parent', '.']\n",
      "['These', 'results', 'are', 'largely', 'confirmed', 'by', 'quantitative', 'absorptions', ',', 'except', 'that', 'no', 'diminution', 'of', 'H-2k', 'expression', 'is', 'observed', ',', 'suggesting', 'that', 'fusion', 'with', 'the', 'TerC', 'parents', 'may', 'transfer', 'a', 'resistance', 'to', 'complement', 'damage', 'to', 'the', 'L-cell', 'parent', '.'] \n",
      "\n",
      "The results show that prostaglandin E2 is effective in promoting the healing of gastric ulcers.\n",
      "['The', 'results', 'show', 'that', 'prostaglandin', 'E2', 'is', 'effective', 'in', 'promoting', 'the', 'healing', 'of', 'gastric', 'ulcers', '.']\n",
      "['The', 'results', 'show', 'that', 'prostaglandin', 'E2', 'is', 'effective', 'in', 'promoting', 'the', 'healing', 'of', 'gastric', 'ulcers', '.'] \n",
      "\n",
      "Some relevant issues are discussed in terms of (a) diagnostic practices, (b) treatment practices, and (c) changing views about the nature of the disorder itself.\n",
      "['Some', 'relevant', 'issues', 'are', 'discussed', 'in', 'terms', 'of', '(', 'a', ')', 'diagnostic', 'practices', ',', '(', 'b', ')', 'treatment', 'practices', ',', 'and', '(', 'c', ')', 'changing', 'views', 'about', 'the', 'nature', 'of', 'the', 'disorder', 'itself', '.']\n",
      "['Some', 'relevant', 'issues', 'are', 'discussed', 'in', 'terms', 'of', '(', 'a', ')', 'diagnostic', 'practices', ',', '(', 'b', ')', 'treatment', 'practices', ',', 'and', '(', 'c', ')', 'changing', 'views', 'about', 'the', 'nature', 'of', 'the', 'disorder', 'itself', '.'] \n",
      "\n",
      "Pup-produced cues from donor litters elicited different patterns of maternal behavior from test mothers that interacted with their own litters.\n",
      "['Pup-produced', 'cues', 'from', 'donor', 'litters', 'elicited', 'different', 'patterns', 'of', 'maternal', 'behavior', 'from', 'test', 'mothers', 'that', 'interacted', 'with', 'their', 'own', 'litters', '.']\n",
      "['Pup-produced', 'cues', 'from', 'donor', 'litters', 'elicited', 'different', 'patterns', 'of', 'maternal', 'behavior', 'from', 'test', 'mothers', 'that', 'interacted', 'with', 'their', 'own', 'litters', '.'] \n",
      "\n",
      "Plasma volume (PV) and extracellular volume (ECV) were measured by T-1824 and [14C]inulin, respectively.\n",
      "['Plasma', 'volume', '(', 'PV', ')', 'and', 'extracellular', 'volume', '(', 'ECV', ')', 'were', 'measured', 'by', 'T-1824', 'and', '[', '14C', ']', 'inulin', ',', 'respectively', '.']\n",
      "['Plasma', 'volume', '(', 'PV', ')', 'and', 'extracellular', 'volume', '(', 'ECV', ')', 'were', 'measured', 'by', 'T-1824', 'and', '[', '14C]inulin', ',', 'respectively', '.'] \n",
      "\n",
      "Ampicillin was given intramuscularly, orally, and as the pro-drug bacampicillin.\n",
      "['Ampicillin', 'was', 'given', 'intramuscularly', ',', 'orally', ',', 'and', 'as', 'the', 'pro-drug', 'bacampicillin', '.']\n",
      "['Ampicillin', 'was', 'given', 'intramuscularly', ',', 'orally', ',', 'and', 'as', 'the', 'pro-drug', 'bacampicillin', '.'] \n",
      "\n",
      "Under the notion short-term therapy on the one hand by the application of the preparations mentioned and on the other hand basing on the experiences in the medicamentous treatment of the pulmonary tuberculosis now in the same measure the possiblity of a total duration of the therapy of on an average twelve months develops for the extrapulmonary manifestations of tuberculosis.\n",
      "['Under', 'the', 'notion', 'short-term', 'therapy', 'on', 'the', 'one', 'hand', 'by', 'the', 'application', 'of', 'the', 'preparations', 'mentioned', 'and', 'on', 'the', 'other', 'hand', 'basing', 'on', 'the', 'experiences', 'in', 'the', 'medicamentous', 'treatment', 'of', 'the', 'pulmonary', 'tuberculosis', 'now', 'in', 'the', 'same', 'measure', 'the', 'possiblity', 'of', 'a', 'total', 'duration', 'of', 'the', 'therapy', 'of', 'on', 'an', 'average', 'twelve', 'months', 'develops', 'for', 'the', 'extrapulmonary', 'manifestations', 'of', 'tuberculosis', '.']\n",
      "['Under', 'the', 'notion', 'shor', 't-t', 'erm', 'therapy', 'on', 'the', 'one', 'hand', 'by', 'the', 'application', 'of', 'the', 'preparations', 'mentioned', 'and', 'on', 'the', 'other', 'hand', 'basing', 'on', 'the', 'experiences', 'in', 'the', 'medicamentous', 'treatment', 'of', 'the', 'pulmonary', 'tuberculosis', 'now', 'in', 'the', 'same', 'measure', 'the', 'possiblity', 'of', 'a', 'total', 'duration', 'of', 'the', 'therapy', 'of', 'on', 'an', 'average', 'twelve', 'months', 'develops', 'for', 'the', 'extrapulmonary', 'manifestations', 'of', 'tuberculosis', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleSize = 10\n",
    "\n",
    "sampleSentences = mcr.sample_raw_sents(sampleSize)\n",
    "\n",
    "for sentence in sampleSentences:\n",
    "    print(sentence)\n",
    "    print(word_tokenize(sentence))\n",
    "    print(twitter_tokenize(sentence), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
